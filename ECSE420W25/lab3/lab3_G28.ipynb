{"cells":[{"cell_type":"markdown","metadata":{"id":"OQ-8T1zfKZbv"},"source":["#Section 0: Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1743186851574,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"OlzmcQNJQWZC","outputId":"a633c912-ffc7-468a-b152-573d315e2d1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16978,"status":"ok","timestamp":1743186868557,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"XzSv40VwRGzb","outputId":"7d27bb02-3b57-41e2-92e1-aac814b6db23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743186868569,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"VxkK286iKcqw","outputId":"01ad2184-6d67-42e1-f74a-ef5d1cefb72a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing read_input.h\n"]}],"source":["%%writefile read_input.h\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","int read_input_one_two_four(int **input1, char* filepath) {\n"," FILE* fp = fopen(filepath, \"r\");\n","    if (fp == NULL){\n","     fprintf(stderr, \"Couldn't open file for reading\\n\");\n","     exit(1);\n","    }\n","\n","    int counter = 0;\n","    int len;\n","    int length = fscanf(fp, \"%d\", &len);\n","    *input1 = ( int *)malloc(len * sizeof(int));\n","\n","    int temp1;\n","\n","    while (fscanf(fp, \"%d\", &temp1) == 1) {\n","        (*input1)[counter] = temp1;\n","\n","        counter++;\n","    }\n","\n","    fclose(fp);\n","    return len;\n","\n","\n","\n","\n","}\n","int read_input_three(int** input1, int** input2, int** input3, int** input4,char* filepath){\n","    FILE* fp = fopen(filepath, \"r\");\n","    if (fp == NULL){\n","     fprintf(stderr, \"Couldn't open file for reading\\n\");\n","     exit(1);\n","    }\n","\n","    int counter = 0;\n","    int len;\n","    int length = fscanf(fp, \"%d\", &len);\n","    *input1 = ( int *)malloc(len * sizeof(int));\n","    *input2 = ( int *)malloc(len * sizeof(int));\n","    *input3 = ( int *)malloc(len * sizeof(int));\n","    *input4 = ( int *)malloc(len * sizeof(int));\n","\n","\n","\n","    int temp1;\n","    int temp2;\n","    int temp3;\n","    int temp4;\n","    while (fscanf(fp, \"%d,%d,%d,%d\", &temp1, &temp2, &temp3, &temp4) == 4) {\n","        (*input1)[counter] = temp1;\n","        (*input2)[counter] = temp2;\n","        (*input3)[counter] = temp3;\n","        (*input4)[counter] = temp4;\n","        counter++;\n","    }\n","\n","    fclose(fp);\n","    return len;\n","\n","}\n","\n","void write_output_file(const char *filename, int *data, int size) {\n","    FILE *file = fopen(filename, \"w\");\n","    if (!file) {\n","        perror(\"Error opening output file\");\n","        exit(EXIT_FAILURE);\n","    }\n","    fprintf(file, \"%d\\n\", size);\n","    for (int i = 0; i < size; i++) {\n","        fprintf(file, \"%d\\n\", data[i]);\n","    }\n","    fclose(file);\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"5_9SMw-UfuIt"},"source":["# Section 1: Sequential"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1743186868591,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"ySF1_57zROl0","outputId":"b4cedc1a-2ec0-4060-df95-c8f329d3770a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing sequential.c\n"]}],"source":["%%writefile sequential.c\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <stdint.h>\n","#include <time.h>  // For measuring execution time\n","\n","\n","#define MAX_NODES 1000000  // Adjust based on expected graph size\n","#define AND 0\n","#define OR 1\n","#define NAND 2\n","#define NOR 3\n","#define XOR 4\n","#define XNOR 5\n","\n","// Gate Execution Function\n","int execute(int a, int b, int gate) {\n","    switch(gate) {\n","        case AND: return a & b;\n","        case OR: return a | b;\n","        case NAND: return (a & b) == 0;\n","        case NOR: return (a | b) == 0;\n","        case XOR: return a ^ b;\n","        case XNOR: return (a ^ b) == 0;\n","        default: return 0;\n","    }\n","}\n","\n","// BFS Traversal\n","void bfs(int *nodePtrs, int *nodeNeighbors, int *nodeVisited, int *nodeOutput,\n","         int *nodeGate, int *nodeInput, int *currLevelNodes, int numCurrLevelNodes,\n","         int *nextLevelNodes, int *numNextLevelNodes) {\n","    *numNextLevelNodes = 0;\n","\n","    for (int idx = 0; idx < numCurrLevelNodes; idx++) {\n","        int node = currLevelNodes[idx];\n","\n","        // Iterate over all neighbors\n","        for (int nbrIdx = nodePtrs[node]; nbrIdx < nodePtrs[node + 1]; nbrIdx++) {\n","            int neighbor = nodeNeighbors[nbrIdx];\n","\n","            // If the neighbor hasn't been visited yet\n","            if (!nodeVisited[neighbor]) {\n","                nodeVisited[neighbor] = 1;  // Mark as visited\n","\n","                // Compute output using execute function\n","                nodeOutput[neighbor] = execute(nodeOutput[node], nodeInput[neighbor], nodeGate[neighbor]);\n","                //printf(\"Node %d input 1: %d input 2:  %d gate: %d output: %d\\n\", neighbor, nodeOutput[node], nodeInput[neighbor], nodeGate[neighbor], nodeOutput[neighbor]);\n","                // Add neighbor to next level queue\n","                nextLevelNodes[*numNextLevelNodes] = neighbor;\n","                (*numNextLevelNodes)++;\n","            }\n","        }\n","    }\n","}\n","\n","// Function to read single-column input\n","int read_input_one_two_four(int **data, char* filepath) {\n","    FILE* fp = fopen(filepath, \"r\");\n","    if (!fp) {\n","        fprintf(stderr, \"Couldn't open file %s for reading\\n\", filepath);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int len;\n","    if (fscanf(fp, \"%d\", &len) != 1) {\n","        fprintf(stderr, \"Error reading size from %s\\n\", filepath);\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    *data = (int *)malloc(len * sizeof(int));\n","    if (!*data) {\n","        perror(\"Memory allocation failed\");\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int i = 0; i < len; i++) {\n","        if (fscanf(fp, \"%d\", &((*data)[i])) != 1) {\n","            fprintf(stderr, \"Error reading data from %s\\n\", filepath);\n","            fclose(fp);\n","            exit(EXIT_FAILURE);\n","        }\n","    }\n","\n","    fclose(fp);\n","    return len;\n","}\n","\n","// Function to read four-column input\n","int read_input_three(int **input1, int **input2, int **input3, int **input4, char* filepath) {\n","    FILE* fp = fopen(filepath, \"r\");\n","    if (!fp) {\n","        fprintf(stderr, \"Couldn't open file %s for reading\\n\", filepath);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int len;\n","    if (fscanf(fp, \"%d\", &len) != 1) {\n","        fprintf(stderr, \"Error reading size from %s\\n\", filepath);\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    *input1 = (int *)malloc(len * sizeof(int));\n","    *input2 = (int *)malloc(len * sizeof(int));\n","    *input3 = (int *)malloc(len * sizeof(int));\n","    *input4 = (int *)malloc(len * sizeof(int));\n","\n","    if (!*input1 || !*input2 || !*input3 || !*input4) {\n","        perror(\"Memory allocation failed\");\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int i = 0; i < len; i++) {\n","        int read = fscanf(fp, \"%d,%d,%d,%d\", &((*input1)[i]), &((*input2)[i]), &((*input3)[i]), &((*input4)[i]));\n","        if (read != 4) {\n","            fprintf(stderr, \"Error reading line %d in %s. Read %d values instead of 4.\\n\", i, filepath, read);\n","            exit(EXIT_FAILURE);\n","        }\n","    }\n","\n","    fclose(fp);\n","    return len;\n","}\n","\n","// Function to write output to a file\n","void write_output_file(const char *filename, int *data, int size) {\n","    FILE *file = fopen(filename, \"w\");\n","    if (!file) {\n","        perror(\"Error opening output file\");\n","        exit(EXIT_FAILURE);\n","    }\n","    fprintf(file, \"%d\\n\", size);\n","    for (int i = 0; i < size; i++) {\n","        fprintf(file, \"%d\\n\", data[i]);\n","    }\n","    fclose(file);\n","}\n","\n","// Main Function\n","int main(int argc, char *argv[]) {\n","    if (argc != 7) {\n","        fprintf(stderr, \"Usage: %s <input1.raw> <input2.raw> <input3.raw> <input4.raw> <output1.raw> <output2.raw>\\n\", argv[0]);\n","        return EXIT_FAILURE;\n","    }\n","\n","    // Variables\n","    int numNodePtrs;\n","    int numNodes;\n","    int *nodePtrs_h;\n","    int *nodeNeighbors_h;\n","    int *nodeVisited_h;\n","    int numTotalNeighbors_h;\n","    int *currLevelNodes_h;\n","    int numCurrLevelNodes;\n","    int *nodeGate_h;\n","    int *nodeInput_h;\n","    int *nodeOutput_h;\n","\n","    // Read input files\n","    numNodePtrs = read_input_one_two_four(&nodePtrs_h, argv[1]);\n","    numTotalNeighbors_h = read_input_one_two_four(&nodeNeighbors_h, argv[2]);\n","    numNodes = read_input_three(&nodeVisited_h, &nodeGate_h, &nodeInput_h, &nodeOutput_h, argv[3]);\n","    numCurrLevelNodes = read_input_one_two_four(&currLevelNodes_h, argv[4]);\n","\n","    // Allocate memory for BFS\n","    int *nextLevelNodes = (int *)malloc(numNodes * sizeof(int));\n","    if (!nextLevelNodes) {\n","        perror(\"Memory allocation failed for nextLevelNodes\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int numNextLevelNodes = 0;\n","\n","    // Start timing\n","    clock_t start, end;\n","    double cpu_time_used;\n","    start = clock();\n","\n","    // Perform BFS traversal\n","    bfs(nodePtrs_h, nodeNeighbors_h, nodeVisited_h, nodeOutput_h, nodeGate_h, nodeInput_h,\n","        currLevelNodes_h, numCurrLevelNodes, nextLevelNodes, &numNextLevelNodes);\n","\n","    end = clock();\n","    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC * 1000;\n","\n","    printf(\"Sequential BFS Execution Time: %f milliseconds\\n\", cpu_time_used);\n","\n","\n","    // Write outputs\n","    write_output_file(argv[5], nodeOutput_h, numNodes);\n","    write_output_file(argv[6], nextLevelNodes, numNextLevelNodes);\n","\n","    // Free allocated memory\n","    free(nodePtrs_h);\n","    free(nodeNeighbors_h);\n","    free(nodeVisited_h);\n","    free(nodeGate_h);\n","    free(nodeInput_h);\n","    free(nodeOutput_h);\n","    free(currLevelNodes_h);\n","    free(nextLevelNodes);\n","\n","    return EXIT_SUCCESS;\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"t66Y_SyubwYo","executionInfo":{"status":"ok","timestamp":1743186869095,"user_tz":240,"elapsed":503,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["!gcc sequential.c -o sequential"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1206,"status":"ok","timestamp":1743186870301,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"YYaVH2IpZOQJ","outputId":"8ad0540a-7b5e-4fbb-939d-cb6d0238c5a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["200000\n","0,4,1,-1\n","0,5,1,-1\n","0,0,1,-1\n","0,2,1,-1\n","0,1,1,-1\n","0,0,0,-1\n","0,4,1,-1\n","1,3,1,0\n","0,2,1,-1\n"]}],"source":["!head /content/drive/MyDrive/ECSE420/lab3/input3.raw"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2359,"status":"ok","timestamp":1743186872660,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"ZGvrM5nGdXVi","outputId":"c8b37a41-7d85-41e9-ff07-2d9e29329b22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential BFS Execution Time: 2.017000 milliseconds\n"]}],"source":["!rm -r /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw\n","!rm -r /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes.raw\n","\n","!./sequential /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes.raw"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"F1AXw21Ydq38","executionInfo":{"status":"ok","timestamp":1743186873178,"user_tz":240,"elapsed":510,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["!gcc /content/drive/MyDrive/ECSE420/lab3/compareNextLevelNodes.c -o compareNextLevelNodes"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10283,"status":"ok","timestamp":1743186883462,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"GyY4lWS_eDM8","outputId":"d499cfbe-d71b-491e-934c-3e4df83e9d5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["No errors!\n"]}],"source":["!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"mHFZ8_O-Zm2Z","executionInfo":{"status":"ok","timestamp":1743186883960,"user_tz":240,"elapsed":499,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["!gcc /content/drive/MyDrive/ECSE420/lab3/compareNodeOutput.c -o compareNodeOutput"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":582,"status":"ok","timestamp":1743186884541,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"tY_SdniVaAAY","outputId":"45a16374-9ca6-47f8-b990-8815a6f46c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw <==\n","200000\n","1\n","0\n","1\n","1\n","1\n","0\n","0\n","0\n","1\n","\n","==> /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw <==\n","200000\n","1\n","0\n","1\n","1\n","1\n","0\n","0\n","0\n","1\n"]}],"source":["!head /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":402,"status":"ok","timestamp":1743186884943,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"a6CmHno6cWR1","outputId":"5245f0b3-91f7-4fa0-c26e-b9e4203bf4eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw <==\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","\n","==> /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw <==\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n","-1\n"]}],"source":["!tail /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1743186885376,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"ZPP8sxVSZqcU","outputId":"b4d700da-3066-4f16-bb24-5b5eaabf79ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Errors : 0\t"]}],"source":["!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"markdown","metadata":{"id":"wFLVmND4f0l9"},"source":["#Section 2: Global Queueing\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1743186885409,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"1p9P0qIef9Ug","outputId":"4be592cd-e7f0-4983-d1e0-ae50b0b25349"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing global_queueing.cu\n"]}],"source":["%%writefile global_queueing.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","\n","#define MAX_NODES 1000000\n","#define AND 0\n","#define OR 1\n","#define NAND 2\n","#define NOR 3\n","#define XOR 4\n","#define XNOR 5\n","\n","// Device Function: Gate Execution\n","__device__ int execute(int a, int b, int gate) {\n","    int result;\n","    switch(gate) {\n","        case AND: result = a & b; break;\n","        case OR: result = a | b; break;\n","        case NAND: result = (a & b) == 0; break;\n","        case NOR: result = (a | b) == 0; break;\n","        case XOR: result = a ^ b; break;\n","        case XNOR: result = (a ^ b) == 0; break;\n","        default: result = 0;\n","    }\n","\n","    if (result == -1) {\n","        printf(\"WARNING: execute() returned -1 for Inputs %d, %d, Gate %d\\n\", a, b, gate);\n","    }\n","    return result;\n","}\n","/*\n","__global__ void bfs_kernel(int *nodePtrs, int *nodeNeighbors, int *nodeVisited,\n","                           int *nodeOutput, int *nodeGate, int *nodeInput,\n","                           int *currLevelNodes, int numCurrLevelNodes,\n","                           int *nextLevelNodes, int *numNextLevelNodes) {\n","\n","\n","                          int idx = blockIdx.x * blockDim.x + threadIdx.x; // global thread index that corresponds to current node\n","\n","                          if (threadIdx.x == 0 && blockIdx.x == 0) {\n","                              printf(\"\\n\");\n","                              printf(\"CUDA Kernel Executing with numCurrLevelNodes = %d, Total Threads = %d\\n\",\n","                              numCurrLevelNodes, gridDim.x * blockDim.x);\n","                          }\n","\n","                          //if (threadIdx.x == 0 && blockIdx.x == 0) {\n","                            //for (int i = 0; i < 10; i++) {\n","                            //    printf(\"GPU Output[%d] Before Copy: %d\\n\", i, nodeOutput[i]);\n","                           // }\n","                          //}\n","\n","                          if (idx >= numCurrLevelNodes) return; //check that we are within the boundaries\n","                          for (int i = idx; i < numCurrLevelNodes; i += gridDim.x * blockDim.x) {\n","                            int node = currLevelNodes[i]; // find responsible node\n","\n","                            for (int nbrIdx = nodePtrs[node]; nbrIdx < nodePtrs[node + 1]; nbrIdx++) { //loop over all the neighbors\n","                                int neighbor = nodeNeighbors[nbrIdx];\n","                                if (atomicCAS(&nodeVisited[neighbor], 0, 1) == 0) {\n","                                  nodeOutput[neighbor] = execute(nodeOutput[node], nodeInput[neighbor], nodeGate[neighbor]);\n","\n","                                  int nextIdx = atomicAdd(numNextLevelNodes, 1);\n","                                  nextLevelNodes[nextIdx] = neighbor;\n","\n","\n","                                }\n","\n","                                }\n","                                }\n","}\n","*/\n","__global__ void bfs_kernel(int *nodePtrs, int *nodeNeighbors, int *nodeVisited,\n","                           int *nodeOutput, int *nodeGate, int *nodeInput,\n","                           int *currLevelNodes, int numCurrLevelNodes,\n","                           int *nextLevelNodes, int *numNextLevelNodes) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\n","    if (idx >= numCurrLevelNodes) return; // Bounds check\n","\n","    for (int i = idx; i < numCurrLevelNodes; i += gridDim.x * blockDim.x) {\n","        int node = currLevelNodes[i]; // Responsible node\n","\n","        for (int nbrIdx = nodePtrs[node]; nbrIdx < nodePtrs[node + 1]; nbrIdx++) { // Loop over neighbors\n","            int neighbor = nodeNeighbors[nbrIdx];\n","\n","            if (atomicCAS(&nodeVisited[neighbor], 0, 1) == 0) { // Ensure exclusive access\n","                // Atomic update of node output\n","                atomicExch(&nodeOutput[neighbor], execute(nodeOutput[node], nodeInput[neighbor], nodeGate[neighbor]));\n","\n","                // Atomically add to next level queue\n","                int nextIdx = atomicAdd(numNextLevelNodes, 1);\n","                nextLevelNodes[nextIdx] = neighbor;\n","\n","                __threadfence(); // Memory consistency barrier\n","            }\n","        }\n","\n","\n","    }\n","}\n","\n","\n","// Function to read single-column input\n","int read_input_one_two_four(int **data, char* filepath) {\n","    FILE* fp = fopen(filepath, \"r\");\n","    if (!fp) {\n","        fprintf(stderr, \"Couldn't open file %s for reading\\n\", filepath);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int len;\n","    if (fscanf(fp, \"%d\", &len) != 1) {\n","        fprintf(stderr, \"Error reading size from %s\\n\", filepath);\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    *data = (int *)malloc(len * sizeof(int));\n","    if (!*data) {\n","        perror(\"Memory allocation failed\");\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int i = 0; i < len; i++) {\n","        if (fscanf(fp, \"%d\", &((*data)[i])) != 1) {\n","            fprintf(stderr, \"Error reading data from %s\\n\", filepath);\n","            fclose(fp);\n","            exit(EXIT_FAILURE);\n","        }\n","    }\n","\n","    fclose(fp);\n","    return len;\n","}\n","\n","// Function to read four-column input\n","int read_input_three(int **input1, int **input2, int **input3, int **input4, char* filepath) {\n","    FILE* fp = fopen(filepath, \"r\");\n","    if (!fp) {\n","        fprintf(stderr, \"Couldn't open file %s for reading\\n\", filepath);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int len;\n","    if (fscanf(fp, \"%d\", &len) != 1) {\n","        fprintf(stderr, \"Error reading size from %s\\n\", filepath);\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    *input1 = (int *)malloc(len * sizeof(int));\n","    *input2 = (int *)malloc(len * sizeof(int));\n","    *input3 = (int *)malloc(len * sizeof(int));\n","    *input4 = (int *)malloc(len * sizeof(int));\n","\n","    if (!*input1 || !*input2 || !*input3 || !*input4) {\n","        perror(\"Memory allocation failed\");\n","        fclose(fp);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int i = 0; i < len; i++) {\n","        int read = fscanf(fp, \"%d,%d,%d,%d\", &((*input1)[i]), &((*input2)[i]), &((*input3)[i]), &((*input4)[i]));\n","        if (read != 4) {\n","            fprintf(stderr, \"Error reading line %d in %s. Read %d values instead of 4.\\n\", i, filepath, read);\n","            exit(EXIT_FAILURE);\n","        }\n","    }\n","\n","    fclose(fp);\n","    return len;\n","}\n","\n","// Function to write output to a file\n","void write_output_file(const char *filename, int *data, int size) {\n","    FILE *file = fopen(filename, \"w\");\n","    if (!file) {\n","        perror(\"Error opening output file\");\n","        exit(EXIT_FAILURE);\n","    }\n","    fprintf(file, \"%d\\n\", size);\n","    for (int i = 0; i < size; i++) {\n","        fprintf(file, \"%d\\n\", data[i]);\n","    }\n","    fclose(file);\n","}\n","\n","\n","int main(int argc, char *argv[]) {\n","    if (argc != 9) {\n","        fprintf(stderr, \"Usage: %s <input1.raw> <input2.raw> <input3.raw> <input4.raw> <output1.raw> <output2.raw> <block_index> <thread_index>\\n\", argv[0]);\n","        return EXIT_FAILURE;\n","    }\n","\n","    // Host Variables\n","    int numNodePtrs;\n","    int numNodes;\n","    int *nodePtrs_h;\n","    int *nodeNeighbors_h;\n","    int *nodeVisited_h;\n","    int numTotalNeighbors_h;\n","    int *currLevelNodes_h;\n","    int numCurrLevelNodes; // Should be an int, NOT a pointer\n","    int *nodeGate_h;\n","    int *nodeInput_h;\n","    int *nodeOutput_h;\n","\n","    // Read Input Files (on CPU)\n","    numNodePtrs = read_input_one_two_four(&nodePtrs_h, argv[1]);\n","    numTotalNeighbors_h = read_input_one_two_four(&nodeNeighbors_h, argv[2]);\n","    numNodes = read_input_three(&nodeVisited_h, &nodeGate_h, &nodeInput_h, &nodeOutput_h, argv[3]);\n","    numCurrLevelNodes = read_input_one_two_four(&currLevelNodes_h, argv[4]);\n","    int block_index = atoi(argv[7]);\n","    int thread_index = atoi(argv[8]);\n","\n","    // Allocate memory for BFS\n","    int *nextLevelNodes_h = (int *)malloc(numNodes * sizeof(int));\n","    if (!nextLevelNodes_h) {\n","        perror(\"Memory allocation failed for nextLevelNodes\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int numNextLevelNodes_h = 0;\n","\n","    // Device Variables\n","    int *d_nodePtrs, *d_nodeNeighbors, *d_nodeVisited, *d_nodeOutput, *d_nodeGate, *d_nodeInput;\n","    int *d_currLevelNodes, *d_nextLevelNodes, *d_numNextLevelNodes;\n","\n","    // Allocate memory on GPU\n","    cudaMalloc(&d_nodePtrs, numNodePtrs * sizeof(int));\n","    cudaMalloc(&d_nodeNeighbors, numTotalNeighbors_h * sizeof(int));\n","    cudaMalloc(&d_nodeVisited, numNodes * sizeof(int));\n","    cudaMalloc(&d_nodeOutput, numNodes * sizeof(int));\n","    cudaMalloc(&d_nodeGate, numNodes * sizeof(int));\n","    cudaMalloc(&d_nodeInput, numNodes * sizeof(int));\n","    cudaMalloc(&d_currLevelNodes, numCurrLevelNodes * sizeof(int));\n","    cudaMalloc(&d_nextLevelNodes, numNodes * sizeof(int));\n","    cudaMalloc(&d_numNextLevelNodes, sizeof(int));\n","\n","    // Copy data from host to device\n","    cudaError_t err = cudaMemcpy(d_nodePtrs, nodePtrs_h, numNodePtrs * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_nodeNeighbors, nodeNeighbors_h, numTotalNeighbors_h * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_nodeVisited, nodeVisited_h, numNodes * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_nodeOutput, nodeOutput_h, numNodes * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_nodeGate, nodeGate_h, numNodes * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_nodeInput, nodeInput_h, numNodes * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(d_currLevelNodes, currLevelNodes_h, numCurrLevelNodes * sizeof(int), cudaMemcpyHostToDevice);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemset(d_numNextLevelNodes, 0, sizeof(int));\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","\n","    int numValuesToCheck = 10;\n","    int *tempArray = (int *)malloc(numValuesToCheck * sizeof(int));\n","\n","    cudaMemcpy(tempArray, d_nodeOutput, numValuesToCheck * sizeof(int), cudaMemcpyDeviceToHost);\n","\n","    int threadsPerBlock[] = {32,64,128};  // Can experiment with 32, 64, 128\n","    int maxBlocks[] = {10, 25, 35};  // Lab-specified limits\n","    //int block_index = 2;  // Choose 0 for 5 blocks, 1 for 10 blocks, 2 for 25 blocks\n","    //int thread_index = 2;\n","    int blocksPerGrid = ((numCurrLevelNodes + threadsPerBlock[thread_index] - 1) / threadsPerBlock[thread_index]);\n","    blocksPerGrid = (blocksPerGrid < maxBlocks[block_index]) ? blocksPerGrid : maxBlocks[block_index];\n","\n","    // CUDA Event Timers\n","    cudaEvent_t start, stop;\n","    float milliseconds = 0;\n","    cudaEventCreate(&start);\n","    cudaEventCreate(&stop);\n","\n","\n","    // Launch Kernel with Timing\n","    cudaEventRecord(start);\n","\n","    bfs_kernel<<<blocksPerGrid, threadsPerBlock[thread_index]>>>(\n","    d_nodePtrs, d_nodeNeighbors, d_nodeVisited,\n","    d_nodeOutput, d_nodeGate, d_nodeInput,\n","    d_currLevelNodes, numCurrLevelNodes,\n","    d_nextLevelNodes, d_numNextLevelNodes);\n","\n","    cudaDeviceSynchronize();  // Ensure GPU work is finished before checking\n","\n","\n","    cudaEventRecord(stop);\n","    cudaEventSynchronize(stop);\n","\n","\n","    // Print GPU output after kernel execution\n","\n","\n","    cudaMemcpy(tempArray, d_nodeOutput, numValuesToCheck * sizeof(int), cudaMemcpyDeviceToHost);\n","\n","\n","    cudaEventElapsedTime(&milliseconds, start, stop);\n","    printf(\"Execution Time for %d threads and %d blocks: %f ms\\n\",threadsPerBlock[thread_index] ,maxBlocks[block_index], milliseconds);\n","\n","    // Copy results back to host\n","    err = cudaMemcpy(nextLevelNodes_h, d_nextLevelNodes, numNodes * sizeof(int), cudaMemcpyDeviceToHost);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(&numNextLevelNodes_h, d_numNextLevelNodes, sizeof(int), cudaMemcpyDeviceToHost);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(nodeOutput_h, d_nodeOutput, numNodes * sizeof(int), cudaMemcpyDeviceToHost);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","    err = cudaMemcpy(nodeVisited_h, d_nodeVisited, numNodes * sizeof(int), cudaMemcpyDeviceToHost);\n","    if (err != cudaSuccess) {\n","        printf(\"CUDA memcpy error: %s\\n\", cudaGetErrorString(err));\n","    }\n","\n","\n","\n","\n","\n","    // Write output files\n","    write_output_file(argv[5], nodeOutput_h, numNodes);\n","    write_output_file(argv[6], nextLevelNodes_h, numNextLevelNodes_h);\n","\n","    // Free Device Memory\n","    cudaFree(d_nodePtrs);\n","    cudaFree(d_nodeNeighbors);\n","    cudaFree(d_nodeVisited);\n","    cudaFree(d_nodeOutput);\n","    cudaFree(d_nodeGate);\n","    cudaFree(d_nodeInput);\n","    cudaFree(d_currLevelNodes);\n","    cudaFree(d_nextLevelNodes);\n","    cudaFree(d_numNextLevelNodes);\n","\n","    // Free Host Memory\n","    free(nodePtrs_h);\n","    free(nodeNeighbors_h);\n","    free(nodeVisited_h);\n","    free(nodeGate_h);\n","    free(nodeInput_h);\n","    free(nodeOutput_h);\n","    free(currLevelNodes_h);\n","    free(nextLevelNodes_h);\n","\n","    return EXIT_SUCCESS;\n","}\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"SbaOTSmefSk9","executionInfo":{"status":"ok","timestamp":1743186885410,"user_tz":240,"elapsed":1,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["# 128 35 3.703ms"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3082,"status":"ok","timestamp":1743186888500,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"Cnl69Q0xkv9J","outputId":"844aa475-222d-41a5-face-492294bfff29"},"outputs":[{"output_type":"stream","name":"stdout","text":["ptxas info    : 59 bytes gmem, 16 bytes cmem[4]\n","ptxas info    : Compiling entry function '_Z10bfs_kernelPiS_S_S_S_S_S_iS_S_' for 'sm_75'\n","ptxas info    : Function properties for _Z10bfs_kernelPiS_S_S_S_S_S_iS_S_\n","    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 34 registers, 16 bytes cumulative stack size, 432 bytes cmem[0], 28 bytes cmem[2]\n"]}],"source":["!nvcc --ptxas-options=-v -arch=sm_75 -gencode=arch=compute_75,code=sm_75 global_queueing.cu -o global_queueing\n","!gcc /content/drive/MyDrive/ECSE420/lab3/compareNextLevelNodes.c -o compareNextLevelNodes\n","!gcc /content/drive/MyDrive/ECSE420/lab3/compareNodeOutput.c -o compareNodeOutput"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13077,"status":"ok","timestamp":1743186901593,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"ENihpRxQlExX","outputId":"18c46129-d266-4d13-beed-2484b700dbf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 32 threads and 10 blocks: 3.463296 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!rm -rf /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw\n","!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 0\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3559,"status":"ok","timestamp":1743186905153,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"0JcSszc6YaVp","outputId":"757d3783-2950-493f-8e41-499c88916d76","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 828 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 32 threads and 10 blocks: 1670.822388 ms\n","==PROF== Disconnected from process 828\n","[828] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (10, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.99\n","    SM Frequency                    Mhz       584.93\n","    Elapsed Cycles                cycle      399,472\n","    Memory Throughput                 %         1.70\n","    DRAM Throughput                   %         1.07\n","    Duration                         us       682.94\n","    L1/TEX Cache Throughput           %         6.77\n","    L2 Cache Throughput               %         1.70\n","    SM Active Cycles              cycle    96,322.68\n","    Compute (SM) Throughput           %         0.51\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         2.10\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.04\n","    Executed Ipc Elapsed  inst/cycle         0.01\n","    Issue Slots Busy               %         1.09\n","    Issued Ipc Active     inst/cycle         0.04\n","    SM Busy                        %         1.09\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 99.38%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         3.43\n","    Mem Busy                    %         1.70\n","    Max Bandwidth               %         1.68\n","    L1/TEX Hit Rate             %         1.89\n","    L2 Hit Rate                 %        91.27\n","    Mem Pipes Busy              %         0.51\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 2.051%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.5321%                                                                                         \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.2 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.35\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.65\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.65%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.0 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.01\n","    Warp Cycles Per Executed Instruction           cycle        23.90\n","    Avg. Active Threads Per Warp                                20.39\n","    Avg. Not Predicated Off Threads Per Warp                    19.37\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.57%                                                                                          \n","          On average, each warp of this kernel spends 11.6 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.6% of the total average of 23.0 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.2003%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,007.14\n","    Executed Instructions                           inst      161,143\n","    Avg. Issued Instructions Per Scheduler          inst     1,046.27\n","    Issued Instructions                             inst      167,403\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     10\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread             320\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.02\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 75%                                                                                             \n","          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           48\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.13\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n","          memory.                                                                                                       \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       36,599\n","    Total DRAM Elapsed Cycles        cycle   27,279,360\n","    Average L1 Active Cycles         cycle    96,322.68\n","    Total L1 Elapsed Cycles          cycle   15,979,840\n","    Average L2 Active Cycles         cycle   227,733.50\n","    Total L2 Elapsed Cycles          cycle   18,682,912\n","    Average SM Active Cycles         cycle    96,322.68\n","    Total SM Elapsed Cycles          cycle   15,979,840\n","    Average SMSP Active Cycles       cycle    24,074.38\n","    Total SMSP Elapsed Cycles        cycle   63,919,360\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 18.28%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 75.83% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 5.662%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 93.95% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 18.28%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 75.83% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,925\n","    Branch Efficiency                   %        60.61\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 31.43%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219520 excessive sectors (81% of the      \n","          total 272470 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 0\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11521,"status":"ok","timestamp":1743186916675,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"uyAuirEXVJgj","outputId":"92a01434-e642-47ae-a1cf-a635eb2f2dd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 64 threads and 10 blocks: 3.120736 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 1\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2237,"status":"ok","timestamp":1743186918925,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"4AjNgPifYckH","outputId":"8afc9e94-c526-47c6-cf52-6c790954b392","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 955 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 64 threads and 10 blocks: 1185.221313 ms\n","==PROF== Disconnected from process 955\n","[955] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (10, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.00\n","    SM Frequency                    Mhz       584.87\n","    Elapsed Cycles                cycle      204,470\n","    Memory Throughput                 %         3.19\n","    DRAM Throughput                   %         2.07\n","    Duration                         us       349.60\n","    L1/TEX Cache Throughput           %        13.28\n","    L2 Cache Throughput               %         3.13\n","    SM Active Cycles              cycle    49,066.50\n","    Compute (SM) Throughput           %         0.99\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         1.05\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.08\n","    Executed Ipc Elapsed  inst/cycle         0.02\n","    Issue Slots Busy               %         2.13\n","    Issued Ipc Active     inst/cycle         0.09\n","    SM Busy                        %         2.13\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.79%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         6.63\n","    Mem Busy                    %         3.13\n","    Max Bandwidth               %         3.19\n","    L1/TEX Hit Rate             %         2.22\n","    L2 Hit Rate                 %        91.13\n","    Mem Pipes Busy              %         0.99\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 4.008%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.039%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.2 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.29\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.71\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.71%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.3 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.30\n","    Warp Cycles Per Executed Instruction           cycle        24.20\n","    Avg. Active Threads Per Warp                                20.40\n","    Avg. Not Predicated Off Threads Per Warp                    19.37\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.88%                                                                                          \n","          On average, each warp of this kernel spends 11.9 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.9% of the total average of 23.3 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.3919%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,007.39\n","    Executed Instructions                           inst      161,183\n","    Avg. Issued Instructions Per Scheduler          inst     1,046.52\n","    Issued Instructions                             inst      167,443\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     10\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread             640\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.02\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 75%                                                                                             \n","          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           24\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.22\n","    Achieved Active Warps Per SM           warp         1.99\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.78%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       36,225\n","    Total DRAM Elapsed Cycles        cycle   13,987,840\n","    Average L1 Active Cycles         cycle    49,066.50\n","    Total L1 Elapsed Cycles          cycle    8,163,488\n","    Average L2 Active Cycles         cycle   170,552.28\n","    Total L2 Elapsed Cycles          cycle    9,562,848\n","    Average SM Active Cycles         cycle    49,066.50\n","    Total SM Elapsed Cycles          cycle    8,163,488\n","    Average SMSP Active Cycles       cycle    24,394.70\n","    Total SMSP Elapsed Cycles        cycle   32,653,952\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 18.23%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 75.83% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 10.5%                                                                                           \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 87.87% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 18.23%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 75.83% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 6.291%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 11.02% above the average, while the minimum instance value is 2.77% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,915\n","    Branch Efficiency                   %        60.60\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 45.98%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219526 excessive sectors (81% of the      \n","          total 272476 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 1\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9721,"status":"ok","timestamp":1743186928647,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"eq5yeavEVJvo","outputId":"ec9c30a2-df01-4b92-adab-5302491a899e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 128 threads and 10 blocks: 3.555456 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 2\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2405,"status":"ok","timestamp":1743186931053,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"f2l-RoaIYXxt","outputId":"8f87bc9c-8be6-4372-b4a6-fc36505010dd","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1076 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 128 threads and 10 blocks: 1159.676880 ms\n","==PROF== Disconnected from process 1076\n","[1076] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (10, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.98\n","    SM Frequency                    Mhz       584.79\n","    Elapsed Cycles                cycle      107,529\n","    Memory Throughput                 %         6.08\n","    DRAM Throughput                   %         3.80\n","    Duration                         us       183.87\n","    L1/TEX Cache Throughput           %        25.21\n","    L2 Cache Throughput               %         5.88\n","    SM Active Cycles              cycle    25,816.65\n","    Compute (SM) Throughput           %         1.89\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       524.29\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.16\n","    Executed Ipc Elapsed  inst/cycle         0.04\n","    Issue Slots Busy               %         4.06\n","    Issued Ipc Active     inst/cycle         0.16\n","    SM Busy                        %         4.06\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 97.7%                                                                                     \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        12.11\n","    Mem Busy                    %         5.88\n","    Max Bandwidth               %         6.08\n","    L1/TEX Hit Rate             %         2.27\n","    L2 Hit Rate                 %        91.45\n","    Mem Pipes Busy              %         1.89\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 7.625%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.973%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.2 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.12\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.88\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 93.92%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 24.3 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        24.37\n","    Warp Cycles Per Executed Instruction           cycle        25.32\n","    Avg. Active Threads Per Warp                                20.40\n","    Avg. Not Predicated Off Threads Per Warp                    19.38\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 51.67%                                                                                          \n","          On average, each warp of this kernel spends 12.6 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 51.7% of the total average of 24.4 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.7464%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,007.89\n","    Executed Instructions                           inst      161,263\n","    Avg. Issued Instructions Per Scheduler          inst     1,047.02\n","    Issued Instructions                             inst      167,523\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                   128\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     10\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,280\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.03\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 75%                                                                                             \n","          The grid for this launch is configured to execute only 10 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           12\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block            8\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %        12.36\n","    Achieved Active Warps Per SM           warp         3.96\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 87.64%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.4%) can be the     \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       34,793\n","    Total DRAM Elapsed Cycles        cycle    7,331,840\n","    Average L1 Active Cycles         cycle    25,816.65\n","    Total L1 Elapsed Cycles          cycle    4,282,904\n","    Average L2 Active Cycles         cycle   122,245.06\n","    Total L2 Elapsed Cycles          cycle    5,028,928\n","    Average SM Active Cycles         cycle    25,816.65\n","    Total SM Elapsed Cycles          cycle    4,282,904\n","    Average SMSP Active Cycles       cycle    25,429.13\n","    Total SMSP Elapsed Cycles        cycle   17,131,616\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 18.26%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 75.73% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 18.05%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 75.98% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 18.26%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 75.73% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 5.575%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 7.17% above the average, while the minimum instance value is 1.64% below the        \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,895\n","    Branch Efficiency                   %        60.56\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 62.67%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219522 excessive sectors (81% of the      \n","          total 272472 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 0 2\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9457,"status":"ok","timestamp":1743186940511,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"o_i-OpLqVJ4_","outputId":"a194af71-45e8-4cc4-ac7a-7a6f4d0e1e6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 32 threads and 25 blocks: 2.868000 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 0\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2449,"status":"ok","timestamp":1743186942972,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"M-0wwZKwYfd6","outputId":"3255de1f-18b3-40cc-a40e-dea32b77e1ee","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1193 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 32 threads and 25 blocks: 1163.164307 ms\n","==PROF== Disconnected from process 1193\n","[1193] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (25, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.95\n","    SM Frequency                    Mhz       584.91\n","    Elapsed Cycles                cycle      165,403\n","    Memory Throughput                 %         4.01\n","    DRAM Throughput                   %         2.53\n","    Duration                         us       282.78\n","    L1/TEX Cache Throughput           %         6.78\n","    L2 Cache Throughput               %         4.01\n","    SM Active Cycles              cycle    96,249.12\n","    Compute (SM) Throughput           %         1.23\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       524.29\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.04\n","    Executed Ipc Elapsed  inst/cycle         0.02\n","    Issue Slots Busy               %         1.09\n","    Issued Ipc Active     inst/cycle         0.04\n","    SM Busy                        %         1.09\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 99.38%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         8.03\n","    Mem Busy                    %         4.01\n","    Max Bandwidth               %         3.97\n","    L1/TEX Hit Rate             %         1.89\n","    L2 Hit Rate                 %        91.21\n","    Mem Pipes Busy              %         1.23\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 4.989%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.297%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.2 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.35\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.65\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.65%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.0 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.01\n","    Warp Cycles Per Executed Instruction           cycle        23.90\n","    Avg. Active Threads Per Warp                                20.40\n","    Avg. Not Predicated Off Threads Per Warp                    19.37\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.31%                                                                                          \n","          On average, each warp of this kernel spends 11.6 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.3% of the total average of 23.0 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.4871%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,007.52\n","    Executed Instructions                           inst      161,203\n","    Avg. Issued Instructions Per Scheduler          inst     1,046.64\n","    Issued Instructions                             inst      167,463\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread             800\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           48\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.13\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n","          memory.                                                                                                       \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    35,482.50\n","    Total DRAM Elapsed Cycles        cycle   11,207,680\n","    Average L1 Active Cycles         cycle    96,249.12\n","    Total L1 Elapsed Cycles          cycle    6,566,928\n","    Average L2 Active Cycles         cycle   159,793.47\n","    Total L2 Elapsed Cycles          cycle    7,735,808\n","    Average SM Active Cycles         cycle    96,249.12\n","    Total SM Elapsed Cycles          cycle    6,566,928\n","    Average SMSP Active Cycles       cycle    24,082.28\n","    Total SMSP Elapsed Cycles        cycle   26,267,712\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 24.06%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 41.04% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 85.24% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.06%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 41.04% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 5.918%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 8.95% above the average, while the minimum instance value is 2.15% below the        \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,910\n","    Branch Efficiency                   %        60.59\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 53.25%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219507 excessive sectors (81% of the      \n","          total 272457 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 0\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9304,"status":"ok","timestamp":1743186952277,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"b5orp2iHUpLT","outputId":"4b2600e5-2c7d-42f7-ca42-30a488f6de02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 64 threads and 25 blocks: 2.766112 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 1\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2742,"status":"ok","timestamp":1743186955026,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"iiJ75gx1YhB0","outputId":"ff21fc5d-f1c6-46d4-e719-b3a56f8fdee3","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1310 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 64 threads and 25 blocks: 1308.129150 ms\n","==PROF== Disconnected from process 1310\n","[1310] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (25, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.03\n","    SM Frequency                    Mhz       584.76\n","    Elapsed Cycles                cycle       91,075\n","    Memory Throughput                 %         7.14\n","    DRAM Throughput                   %         4.30\n","    Duration                         us       155.74\n","    L1/TEX Cache Throughput           %        13.17\n","    L2 Cache Throughput               %         6.86\n","    SM Active Cycles              cycle    49,457.97\n","    Compute (SM) Throughput           %         2.22\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       524.29\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.08\n","    Executed Ipc Elapsed  inst/cycle         0.04\n","    Issue Slots Busy               %         2.12\n","    Issued Ipc Active     inst/cycle         0.08\n","    SM Busy                        %         2.12\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.8%                                                                                     \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        13.84\n","    Mem Busy                    %         6.86\n","    Max Bandwidth               %         7.14\n","    L1/TEX Hit Rate             %         2.14\n","    L2 Hit Rate                 %        91.26\n","    Mem Pipes Busy              %         2.22\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 8.957%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 2.339%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.1 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.25\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.75\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 92.86%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.6 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.57\n","    Warp Cycles Per Executed Instruction           cycle        24.48\n","    Avg. Active Threads Per Warp                                20.41\n","    Avg. Not Predicated Off Threads Per Warp                    19.38\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.83%                                                                                          \n","          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.8% of the total average of 23.6 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.8754%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,008.14\n","    Executed Instructions                           inst      161,303\n","    Avg. Issued Instructions Per Scheduler          inst     1,047.27\n","    Issued Instructions                             inst      167,563\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,600\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           24\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.23\n","    Achieved Active Warps Per SM           warp         1.99\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 92.86%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    33,687.50\n","    Total DRAM Elapsed Cycles        cycle    6,262,784\n","    Average L1 Active Cycles         cycle    49,457.97\n","    Total L1 Elapsed Cycles          cycle    3,650,240\n","    Average L2 Active Cycles         cycle   103,399.66\n","    Total L2 Elapsed Cycles          cycle    4,259,392\n","    Average SM Active Cycles         cycle    49,457.97\n","    Total SM Elapsed Cycles          cycle    3,650,240\n","    Average SMSP Active Cycles       cycle    24,667.38\n","    Total SMSP Elapsed Cycles        cycle   14,600,960\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 24.48%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 45.17% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 19.65%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 72.69% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.48%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 45.17% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,885\n","    Branch Efficiency                   %        60.55\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 62.59%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219526 excessive sectors (81% of the      \n","          total 272476 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 1\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9105,"status":"ok","timestamp":1743186964150,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"fddL_Jc7VPMT","outputId":"7296a934-39d9-4c16-814e-532956903130"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 128 threads and 25 blocks: 2.689312 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 2\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3140,"status":"ok","timestamp":1743186967291,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"kgWovUsvYigQ","outputId":"b5d31544-63ac-4112-a494-f378f046bc00","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1433 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 128 threads and 25 blocks: 1272.847290 ms\n","==PROF== Disconnected from process 1433\n","[1433] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (25, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.96\n","    SM Frequency                    Mhz       584.68\n","    Elapsed Cycles                cycle       55,943\n","    Memory Throughput                 %        11.63\n","    DRAM Throughput                   %         6.66\n","    Duration                         us        95.68\n","    L1/TEX Cache Throughput           %        24.34\n","    L2 Cache Throughput               %        11.12\n","    SM Active Cycles              cycle    26,712.25\n","    Compute (SM) Throughput           %         3.62\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       262.14\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.15\n","    Executed Ipc Elapsed  inst/cycle         0.07\n","    Issue Slots Busy               %         3.93\n","    Issued Ipc Active     inst/cycle         0.16\n","    SM Busy                        %         3.93\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 97.78%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        21.11\n","    Mem Busy                    %        11.12\n","    Max Bandwidth               %        11.63\n","    L1/TEX Hit Rate             %         2.29\n","    L2 Hit Rate                 %        91.19\n","    Mem Pipes Busy              %         3.62\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 14.54%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 3.79%                                                                                           \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.2 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.00\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        96.00\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 88.37%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 25.0 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        25.05\n","    Warp Cycles Per Executed Instruction           cycle        26.02\n","    Avg. Active Threads Per Warp                                20.42\n","    Avg. Not Predicated Off Threads Per Warp                    19.39\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 51.66%                                                                                          \n","          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 51.7% of the total average of 25.0 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.426%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,009.39\n","    Executed Instructions                           inst      161,503\n","    Avg. Issued Instructions Per Scheduler          inst     1,048.52\n","    Issued Instructions                             inst      167,763\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                   128\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           3,200\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.08\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           12\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block            8\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %        12.29\n","    Achieved Active Warps Per SM           warp         3.93\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 87.71%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.3%) can be the     \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    31,559.50\n","    Total DRAM Elapsed Cycles        cycle    3,792,896\n","    Average L1 Active Cycles         cycle    26,712.25\n","    Total L1 Elapsed Cycles          cycle    2,236,312\n","    Average L2 Active Cycles         cycle    63,754.62\n","    Total L2 Elapsed Cycles          cycle    2,616,352\n","    Average SM Active Cycles         cycle    26,712.25\n","    Total SM Elapsed Cycles          cycle    2,236,312\n","    Average SMSP Active Cycles       cycle    26,234.67\n","    Total SMSP Elapsed Cycles        cycle    8,945,248\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 24.65%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 51.58% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.35%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 51.89% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.65%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 51.58% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,835\n","    Branch Efficiency                   %        60.46\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 62.82%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219534 excessive sectors (81% of the      \n","          total 272484 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 1 2\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8796,"status":"ok","timestamp":1743186976106,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"B0T5e0JZVPaX","outputId":"eecdc0eb-ef8e-4158-c797-edafcbbd0511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 32 threads and 35 blocks: 3.405568 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 0\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1743186979644,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"rb5B-nt2YkKM","outputId":"e745b552-5e54-4869-ccea-e355704cf726","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1552 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 32 threads and 35 blocks: 1654.069214 ms\n","==PROF== Disconnected from process 1552\n","[1552] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (35, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.01\n","    SM Frequency                    Mhz       584.85\n","    Elapsed Cycles                cycle      115,119\n","    Memory Throughput                 %         5.68\n","    DRAM Throughput                   %         3.50\n","    Duration                         us       196.83\n","    L1/TEX Cache Throughput           %         8.68\n","    L2 Cache Throughput               %         5.59\n","    SM Active Cycles              cycle    96,962.15\n","    Compute (SM) Throughput           %         1.76\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       524.29\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.04\n","    Executed Ipc Elapsed  inst/cycle         0.04\n","    Issue Slots Busy               %         1.08\n","    Issued Ipc Active     inst/cycle         0.04\n","    SM Busy                        %         1.08\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 99.39%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        11.22\n","    Mem Busy                    %         5.59\n","    Max Bandwidth               %         5.68\n","    L1/TEX Hit Rate             %         1.89\n","    L2 Hit Rate                 %        91.14\n","    Mem Pipes Busy              %         1.76\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 7.127%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.875%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.1 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.32\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.68\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 94.32%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.1 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.10\n","    Warp Cycles Per Executed Instruction           cycle        24.00\n","    Avg. Active Threads Per Warp                                20.40\n","    Avg. Not Predicated Off Threads Per Warp                    19.37\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.5%                                                                                           \n","          On average, each warp of this kernel spends 11.7 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.5% of the total average of 23.1 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 0.6958%                                                                                         \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,007.77\n","    Executed Instructions                           inst      161,243\n","    Avg. Issued Instructions Per Scheduler          inst     1,046.89\n","    Issued Instructions                             inst      167,503\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,120\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           48\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.12\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.76%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n","          memory.                                                                                                       \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    34,496.50\n","    Total DRAM Elapsed Cycles        cycle    7,884,800\n","    Average L1 Active Cycles         cycle    96,962.15\n","    Total L1 Elapsed Cycles          cycle    4,595,264\n","    Average L2 Active Cycles         cycle   126,908.75\n","    Total L2 Elapsed Cycles          cycle    5,383,968\n","    Average SM Active Cycles         cycle    96,962.15\n","    Total SM Elapsed Cycles          cycle    4,595,264\n","    Average SMSP Active Cycles       cycle    24,206.09\n","    Total SMSP Elapsed Cycles        cycle   18,381,056\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 12.52%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 14.83% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 16.58%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 78.71% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 12.52%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 14.83% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 5.277%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 7.00% above the average, while the minimum instance value is 1.99% below the        \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,900\n","    Branch Efficiency                   %        60.57\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 60.77%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219536 excessive sectors (81% of the      \n","          total 272486 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 0\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9038,"status":"ok","timestamp":1743186988692,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"uHO755lQVPgX","outputId":"0b6246d9-3e98-4119-f8d6-b5b905ce4071"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 64 threads and 35 blocks: 3.400384 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 1\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2466,"status":"ok","timestamp":1743186991160,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"O-tw57O1Yl_K","outputId":"f73e0a7e-d5fc-4db4-cdcb-75c5008c2f6e","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1671 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 64 threads and 35 blocks: 1315.207153 ms\n","==PROF== Disconnected from process 1671\n","[1671] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (35, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.02\n","    SM Frequency                    Mhz       584.61\n","    Elapsed Cycles                cycle       66,114\n","    Memory Throughput                 %         9.82\n","    DRAM Throughput                   %         5.77\n","    Duration                         us       113.09\n","    L1/TEX Cache Throughput           %        14.99\n","    L2 Cache Throughput               %         9.44\n","    SM Active Cycles              cycle    50,257.95\n","    Compute (SM) Throughput           %         3.05\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       262.14\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.08\n","    Executed Ipc Elapsed  inst/cycle         0.06\n","    Issue Slots Busy               %         2.08\n","    Issued Ipc Active     inst/cycle         0.08\n","    SM Busy                        %         2.08\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.82%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        18.53\n","    Mem Busy                    %         9.44\n","    Max Bandwidth               %         9.82\n","    L1/TEX Hit Rate             %         2.12\n","    L2 Hit Rate                 %        90.51\n","    Mem Pipes Busy              %         3.05\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 12.31%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 3.226%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.1 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.19\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        95.81\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 90.18%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 23.9 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        23.91\n","    Warp Cycles Per Executed Instruction           cycle        24.84\n","    Avg. Active Threads Per Warp                                20.41\n","    Avg. Not Predicated Off Threads Per Warp                    19.38\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 50.66%                                                                                          \n","          On average, each warp of this kernel spends 12.1 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 50.7% of the total average of 23.9 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.203%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,008.64\n","    Executed Instructions                           inst      161,383\n","    Avg. Issued Instructions Per Scheduler          inst     1,047.77\n","    Issued Instructions                             inst      167,643\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           2,240\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           24\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.22\n","    Achieved Active Warps Per SM           warp         1.99\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 90.18%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    32,737.50\n","    Total DRAM Elapsed Cycles        cycle    4,542,464\n","    Average L1 Active Cycles         cycle    50,257.95\n","    Total L1 Elapsed Cycles          cycle    2,654,224\n","    Average L2 Active Cycles         cycle    81,616.69\n","    Total L2 Elapsed Cycles          cycle    3,092,032\n","    Average SM Active Cycles         cycle    50,257.95\n","    Total SM Elapsed Cycles          cycle    2,654,224\n","    Average SMSP Active Cycles       cycle    25,027.64\n","    Total SMSP Elapsed Cycles        cycle   10,616,896\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 17.27%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 22.80% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 23.26%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 61.66% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 17.27%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 22.80% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.27\n","    Branch Instructions              inst       42,865\n","    Branch Efficiency                   %        60.51\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 68.05%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219539 excessive sectors (81% of the      \n","          total 272489 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 1\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9505,"status":"ok","timestamp":1743187000666,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"bMfaAjpBVPnJ","outputId":"7cf61f41-b765-43dd-cca2-e769009c1d66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution Time for 128 threads and 35 blocks: 2.740416 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 2\n","!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2268,"status":"ok","timestamp":1743187002935,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"trjPulcAYncS","outputId":"463be879-8784-4702-aa70-a9b5e1924db7","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 1789 (/content/global_queueing)\n","==PROF== Profiling \"bfs_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution Time for 128 threads and 35 blocks: 1114.650879 ms\n","==PROF== Disconnected from process 1789\n","[1789] global_queueing@127.0.0.1\n","  bfs_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *) (35, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.95\n","    SM Frequency                    Mhz       584.57\n","    Elapsed Cycles                cycle       43,065\n","    Memory Throughput                 %        15.11\n","    DRAM Throughput                   %         8.39\n","    Duration                         us        73.66\n","    L1/TEX Cache Throughput           %        23.52\n","    L2 Cache Throughput               %        14.36\n","    SM Active Cycles              cycle    27,636.22\n","    Compute (SM) Throughput           %         4.70\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Kbyte       262.14\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.15\n","    Executed Ipc Elapsed  inst/cycle         0.09\n","    Issue Slots Busy               %         3.80\n","    Issued Ipc Active     inst/cycle         0.15\n","    SM Busy                        %         3.80\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 97.85%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s        26.57\n","    Mem Busy                    %        14.36\n","    Max Bandwidth               %        15.11\n","    L1/TEX Hit Rate             %         2.32\n","    L2 Hit Rate                 %        90.84\n","    Mem Pipes Busy              %         4.70\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 18.87%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.7 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 4.943%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.1 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         3.89\n","    Issued Warp Per Scheduler                        0.04\n","    No Eligible                            %        96.11\n","    Active Warps Per Scheduler          warp         1.01\n","    Eligible Warps Per Scheduler        warp         0.04\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 84.89%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 25.7 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        25.89\n","    Warp Cycles Per Executed Instruction           cycle        26.89\n","    Avg. Active Threads Per Warp                                20.43\n","    Avg. Not Predicated Off Threads Per Warp                    19.41\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 51.84%                                                                                          \n","          On average, each warp of this kernel spends 13.4 cycles being stalled waiting for a scoreboard dependency on  \n","          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n","          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n","          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n","          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n","          used data to shared memory. This stall type represents about 51.8% of the total average of 25.9 cycles        \n","          between issuing two instructions.                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.85%                                                                                           \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 20.4 threads being active per cycle. This is further reduced    \n","          to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     1,010.39\n","    Executed Instructions                           inst      161,663\n","    Avg. Issued Instructions Per Scheduler          inst     1,049.52\n","    Issued Instructions                             inst      167,923\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                   128\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              34\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block               0\n","    Static Shared Memory Per Block        byte/block               0\n","    # SMs                                         SM              40\n","    Threads                                   thread           4,480\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.11\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           12\n","    Block Limit Shared Mem                block           16\n","    Block Limit Warps                     block            8\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %        12.29\n","    Achieved Active Warps Per SM           warp         3.93\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 84.89%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.3%) can be the     \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    30,577.50\n","    Total DRAM Elapsed Cycles        cycle    2,916,352\n","    Average L1 Active Cycles         cycle    27,636.22\n","    Total L1 Elapsed Cycles          cycle    1,720,448\n","    Average L2 Active Cycles         cycle    52,052.16\n","    Total L2 Elapsed Cycles          cycle    2,013,952\n","    Average SM Active Cycles         cycle    27,636.22\n","    Total SM Elapsed Cycles          cycle    1,720,448\n","    Average SMSP Active Cycles       cycle    26,992.96\n","    Total SMSP Elapsed Cycles        cycle    6,881,792\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 21.97%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 34.20% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 21.9%                                                                                           \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 34.89% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 21.97%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 34.20% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.26\n","    Branch Instructions              inst       42,795\n","    Branch Efficiency                   %        60.40\n","    Avg. Divergent Branches                      58.12\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 66.63%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 219528 excessive sectors (81% of the      \n","          total 272478 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}],"source":["!ncu --set full ./global_queueing /content/drive/MyDrive/ECSE420/lab3/input1.raw /content/drive/MyDrive/ECSE420/lab3/input2.raw /content/drive/MyDrive/ECSE420/lab3/input3.raw /content/drive/MyDrive/ECSE420/lab3/input4.raw /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw 2 2\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"9nrtX7HenwAS","executionInfo":{"status":"ok","timestamp":1743187002992,"user_tz":240,"elapsed":56,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["!gcc /content/drive/MyDrive/ECSE420/lab3/compareNextLevelNodes.c -o compareNextLevelNodes"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ZH48kvyDlYGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743187012134,"user_tz":240,"elapsed":9141,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"6629cd29-f9ea-48de-846a-05ba95c24baa"},"outputs":[{"output_type":"stream","name":"stdout","text":["No errors!\n"]}],"source":["!./compareNextLevelNodes /content/drive/MyDrive/ECSE420/lab3/output_nextLevelNodes_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"0Hfi5ExYn5q_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743187012229,"user_tz":240,"elapsed":94,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"32e7c0fb-ad97-42f0-8cd1-d83f919ce20b","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["==> /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw <==\n","200000\n","1\n","0\n","1\n","1\n","1\n","0\n","0\n","0\n","1\n","\n","==> /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw <==\n","200000\n","1\n","0\n","1\n","1\n","1\n","0\n","0\n","0\n","1\n"]}],"source":["!head /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"c8CFfmscliCf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743187012334,"user_tz":240,"elapsed":104,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"90db835a-710e-4c68-9ca4-9e097c407d56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Errors : 0\t"]}],"source":["!./compareNodeOutput /content/drive/MyDrive/ECSE420/lab3/output_nodeOutput_gq.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"markdown","metadata":{"id":"hdcE7XAaFbjJ"},"source":["#Section 3: Block Queueing"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"6K7FteI3GsZQ","executionInfo":{"status":"ok","timestamp":1743187012547,"user_tz":240,"elapsed":212,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}}},"outputs":[],"source":["!gcc /content/drive/MyDrive/ECSE420/lab3/compareNextLevelNodes.c -o compareNextLevelNodes\n","!gcc /content/drive/MyDrive/ECSE420/lab3/compareNodeOutput.c -o compareNodeOutput"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1743187012609,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"xZnlPpZcFkRB","outputId":"8f41b1df-be53-42f2-d21f-3f577d13ac0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing block_queueing.cu\n"]}],"source":["%%writefile block_queueing.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","\n","#include \"read_input.h\"\n","\n","#define MAX_NODES 1000000\n","#define AND 0\n","#define OR 1\n","#define NAND 2\n","#define NOR 3\n","#define XOR 4\n","#define XNOR 5\n","\n","inline cudaError_t checkCudaErr(cudaError_t err, const char* msg) {\n","  if (err != cudaSuccess) {\n","    fprintf(stderr, \"CUDA Runtime error at %s: %s\\n\", msg, cudaGetErrorString(err));\n","  }\n","  return err;\n","}\n","\n","// Device Function: Gate Execution\n","__device__ int execute(int a, int b, int gate) {\n","    int result;\n","    switch(gate) {\n","        case AND: result = a & b; break;\n","        case OR: result = a | b; break;\n","        case NAND: result = (a & b) == 0; break;\n","        case NOR: result = (a | b) == 0; break;\n","        case XOR: result = a ^ b; break;\n","        case XNOR: result = (a ^ b) == 0; break;\n","        default: result = 0;\n","    }\n","\n","    if (result == -1) {\n","        printf(\"WARNING: execute() returned -1 for Inputs %d, %d, Gate %d\\n\", a, b, gate);\n","    }\n","    return result;\n","}\n","\n","__global__ void block_queueing_kernel(int *nodePtrs, int *nodeNeighbors, int *nodeVisited,\n","                                      int *nodeOutput, int *nodeGate, int *nodeInput,\n","                                      int *currLevelNodes, int numCurrLevelNodes,\n","                                      int *nextLevelNodes, int *numNextLevelNodes, int sharedQueueSize) {\n","\n","    // Initialize shared memory queue\n","    extern __shared__ int sharedQueue[];\n","    __shared__ int sharedQueueCounter;\n","\n","    int tid = threadIdx.x;\n","    int bid = blockIdx.x;\n","    int numThreads = blockDim.x;\n","\n","    if (tid == 0) {\n","        sharedQueueCounter = 0;\n","    }\n","\n","    // Loop over all nodes in the current level\n","    for (int i = bid; i < numCurrLevelNodes; i += gridDim.x) {\n","        int node = currLevelNodes[i];\n","        int start = nodePtrs[node];\n","        int end = nodePtrs[node + 1];\n","\n","        // Loop over all neighbors of the node\n","        for (int j = start + tid; j < end; j += numThreads) {\n","            int neighbor = nodeNeighbors[j];\n","\n","            // If the neighbor has not been visited yet\n","            if (atomicCAS(&nodeVisited[neighbor], 0, 1) == 0) {\n","                // Update node output\n","                nodeOutput[neighbor] = execute(nodeOutput[node], nodeInput[neighbor], nodeGate[neighbor]);\n","\n","                // Add it to the block queue\n","                int index = atomicAdd(&sharedQueueCounter, 1);\n","                if (index < sharedQueueSize) {\n","                    sharedQueue[index] = neighbor;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","\n","        // If full, add it to the global queue\n","        if (tid == 0) {\n","            while (sharedQueueCounter >= sharedQueueSize) {\n","                int globalIndex = atomicAdd(numNextLevelNodes, sharedQueueSize);\n","\n","                // Add to global queue\n","                for (int k = 0; k < sharedQueueSize; k++) {\n","                    nextLevelNodes[globalIndex + k] = sharedQueue[k];\n","                }\n","\n","                int remaining = sharedQueueCounter - sharedQueueSize;\n","                for (int k = 0; k < remaining; k++) {\n","                    sharedQueue[k] = sharedQueue[sharedQueueSize + k];\n","                }\n","\n","                sharedQueueCounter = remaining;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // Add remaining elements to the global queue\n","        if (tid == 0 && sharedQueueCounter > 0) {\n","            int globalIndex = atomicAdd(numNextLevelNodes, sharedQueueCounter);\n","            for (int k = 0; k < sharedQueueCounter; k++) {\n","                nextLevelNodes[globalIndex + k] = sharedQueue[k];\n","            }\n","            sharedQueueCounter = 0;\n","        }\n","    }\n","}\n","\n","\n","int main(int argc, char **argv)\n","{\n","    int numBlock = atoi(argv[1]);\n","    int blockSize = atoi(argv[2]);\n","    int sharedQueueSize = atoi(argv[3]);\n","    char *inputPath1 = argv[4];\n","    char *inputPath2 = argv[5];\n","    char *inputPath3 = argv[6];\n","    char *inputPath4 = argv[7];\n","    char *outputNodePath = argv[8];\n","    char *outputNextLevelNodesPath = argv[9];\n","\n","    int numNodePtrs;\n","    int numNodes;\n","    int *nodePtrs_h;\n","    int *nodeNeighbors_h;\n","    int *nodeVisited_h;\n","    int numTotalNeighbors_h;\n","    int *currLevelNodes_h;\n","    int numCurrLevelNodes;\n","    int *nodeGate_h;\n","    int *nodeInput_h;\n","    int *nodeOutput_h;\n","\n","    numNodePtrs = read_input_one_two_four(&nodePtrs_h, inputPath1);\n","    numTotalNeighbors_h = read_input_one_two_four(&nodeNeighbors_h, inputPath2);\n","    numNodes = read_input_three(&nodeVisited_h, &nodeGate_h, &nodeInput_h, &nodeOutput_h, inputPath3);\n","    numCurrLevelNodes = read_input_one_two_four(&currLevelNodes_h, inputPath4);\n","\n","    int *nextLevelNodes_h = (int *)malloc(numNodes * sizeof(int));\n","    if (!nextLevelNodes_h)\n","    {\n","        perror(\"Memory allocation failed for nextLevelNodes\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int numNextLevelNodes_h = 0;\n","\n","    // Device Variables\n","    int *d_nodePtrs, *d_nodeNeighbors, *d_nodeVisited, *d_nodeOutput, *d_nodeGate, *d_nodeInput;\n","    int *d_currLevelNodes, *d_nextLevelNodes, *d_numNextLevelNodes;\n","\n","    // Allocate memory on GPU\n","    checkCudaErr(cudaMalloc(&d_nodePtrs, numNodePtrs * sizeof(int)), \"cudaMalloc nodePtrs\");\n","    checkCudaErr(cudaMalloc(&d_nodeNeighbors, numTotalNeighbors_h * sizeof(int)), \"cudaMalloc nodeNeighbors\");\n","    checkCudaErr(cudaMalloc(&d_nodeVisited, numNodes * sizeof(int)), \"cudaMalloc nodeVisited\");\n","    checkCudaErr(cudaMalloc(&d_nodeOutput, numNodes * sizeof(int)), \"cudaMalloc nodeOutput\");\n","    checkCudaErr(cudaMalloc(&d_nodeGate, numNodes * sizeof(int)), \"cudaMalloc nodeGate\");\n","    checkCudaErr(cudaMalloc(&d_nodeInput, numNodes * sizeof(int)), \"cudaMalloc nodeInput\");\n","    checkCudaErr(cudaMalloc(&d_currLevelNodes, numCurrLevelNodes * sizeof(int)), \"cudaMalloc currLevelNodes\");\n","    checkCudaErr(cudaMalloc(&d_nextLevelNodes, numNodes * sizeof(int)), \"cudaMalloc nextLevelNodes\");\n","    checkCudaErr(cudaMalloc(&d_numNextLevelNodes, sizeof(int)), \"cudaMalloc nextLevelNodes\");\n","\n","    // Copy data from host to device\n","    checkCudaErr(cudaMemcpy(d_nodePtrs, nodePtrs_h, numNodePtrs * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodePtrs\");\n","    checkCudaErr(cudaMemcpy(d_nodeNeighbors, nodeNeighbors_h, numTotalNeighbors_h * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodeNeighbors\");\n","    checkCudaErr(cudaMemcpy(d_nodeVisited, nodeVisited_h, numNodes * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodeVisited\");\n","    checkCudaErr(cudaMemcpy(d_nodeOutput, nodeOutput_h, numNodes * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodeOutput\");\n","    checkCudaErr(cudaMemcpy(d_nodeGate, nodeGate_h, numNodes * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodeGate\");\n","    checkCudaErr(cudaMemcpy(d_nodeInput, nodeInput_h, numNodes * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy nodeInput\");\n","    checkCudaErr(cudaMemcpy(d_currLevelNodes, currLevelNodes_h, numCurrLevelNodes * sizeof(int), cudaMemcpyHostToDevice), \"cudaMemcpy currLevelNodes\");\n","    checkCudaErr(cudaMemset(d_numNextLevelNodes, 0, sizeof(int)), \"cudaMemset nextLevelNodes\");\n","\n","    // CUDA Event Timers\n","    cudaEvent_t start, stop;\n","    float milliseconds = 0;\n","    cudaEventCreate(&start);\n","    cudaEventCreate(&stop);\n","\n","\n","    // Launch Kernel with Timing\n","    cudaEventRecord(start);\n","\n","    // Execute the kernel\n","    block_queueing_kernel<<<numBlock, blockSize, sharedQueueSize * sizeof(int)>>>(\n","        d_nodePtrs, d_nodeNeighbors, d_nodeVisited, d_nodeOutput, d_nodeGate,\n","        d_nodeInput, d_currLevelNodes, numCurrLevelNodes, d_nextLevelNodes,\n","        d_numNextLevelNodes, sharedQueueSize);\n","\n","    checkCudaErr(cudaDeviceSynchronize(), \"CUDA synchronize\");\n","    checkCudaErr(cudaGetLastError(), \"CUDA error\");\n","\n","    cudaEventRecord(stop);\n","    cudaEventSynchronize(stop);\n","    cudaEventElapsedTime(&milliseconds, start, stop);\n","\n","    printf(\"Execution time: %.3f ms\\n\", milliseconds);\n","\n","    // Copy results back to host\n","    checkCudaErr(cudaMemcpy(nextLevelNodes_h, d_nextLevelNodes, numNodes * sizeof(int), cudaMemcpyDeviceToHost), \"cudaMemcpy nextLevelNodes\");\n","    checkCudaErr(cudaMemcpy(&numNextLevelNodes_h, d_numNextLevelNodes, sizeof(int), cudaMemcpyDeviceToHost), \"cudaMemcpy numNextLevelNodes\");\n","    checkCudaErr(cudaMemcpy(nodeOutput_h, d_nodeOutput, numNodes * sizeof(int), cudaMemcpyDeviceToHost), \"cudaMemcpy nodeOutput\");\n","    checkCudaErr(cudaMemcpy(nodeVisited_h, d_nodeVisited, numNodes * sizeof(int), cudaMemcpyDeviceToHost), \"cudaMemcpy nodeVisited\");\n","    write_output_file(outputNodePath, nodeOutput_h, numNodes);\n","    write_output_file(outputNextLevelNodesPath, nextLevelNodes_h, numNextLevelNodes_h);\n","\n","    // Clean up\n","    checkCudaErr(cudaFree(d_nodePtrs), \"cudaFree nodePtrs\");\n","    checkCudaErr(cudaFree(d_nodeNeighbors), \"cudaFree nodeNeighbors\");\n","    checkCudaErr(cudaFree(d_nodeVisited), \"cudaFree nodeVisited\");\n","    checkCudaErr(cudaFree(d_nodeOutput), \"cudaFree nodeOutput\");\n","    checkCudaErr(cudaFree(d_nodeGate), \"cudaFree nodeGate\");\n","    checkCudaErr(cudaFree(d_nodeInput), \"cudaFree nodeInput\");\n","    checkCudaErr(cudaFree(d_currLevelNodes), \"cudaFree currLevelNodes\");\n","    checkCudaErr(cudaFree(d_nextLevelNodes), \"cudaFree nextLevelNodes\");\n","    checkCudaErr(cudaFree(d_numNextLevelNodes), \"cudaFree nextLevelNodes\");\n","\n","    free(nodePtrs_h);\n","    free(nodeNeighbors_h);\n","    free(nodeVisited_h);\n","    free(nodeGate_h);\n","    free(nodeInput_h);\n","    free(nodeOutput_h);\n","\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1476,"status":"ok","timestamp":1743187014086,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"5Yya9hptE70X","outputId":"69ec16ad-6215-4034-b397-14b0183bba52"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'block_queueing': No such file or directory\n","ptxas info    : 59 bytes gmem, 16 bytes cmem[4]\n","ptxas info    : Compiling entry function '_Z21block_queueing_kernelPiS_S_S_S_S_S_iS_S_i' for 'sm_75'\n","ptxas info    : Function properties for _Z21block_queueing_kernelPiS_S_S_S_S_S_iS_S_i\n","    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 44 registers, 16 bytes cumulative stack size, 16 bytes smem, 436 bytes cmem[0], 28 bytes cmem[2]\n"]}],"source":["!rm block_queueing\n","!rm -rf output_nextLevelNodes_bq.raw\n","!rm -rf output_nodeOutput_bq.raw\n","!nvcc --ptxas-options=-v -arch=sm_75 -gencode=arch=compute_75,code=sm_75 block_queueing.cu -o block_queueing"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9639,"status":"ok","timestamp":1743187023726,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"gOIq8BG_Q28o","outputId":"d0aa85e6-ae00-4551-ad37-ad7d343203ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.255 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./block_queueing 25 32 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_32_32.raw \\\n","output_nextLevelNodes_bq_25_32_32.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_25_32_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_25_32_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AJxklPXHUrC","executionInfo":{"status":"ok","timestamp":1743187033392,"user_tz":240,"elapsed":9656,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"aa8451fd-0de5-4e2a-a195-9b83dd40f70e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.846 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./block_queueing 25 32 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_32_64.raw \\\n","output_nextLevelNodes_bq_25_32_64.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_25_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_25_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","source":["!./block_queueing 25 64 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_64_32.raw \\\n","output_nextLevelNodes_bq_25_64_32.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_25_64_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_25_64_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAn0Z5g8crsP","executionInfo":{"status":"ok","timestamp":1743187042215,"user_tz":240,"elapsed":8822,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"b590f193-d497-4053-d4af-28178564b0e1"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 9.050 ms\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9552,"status":"ok","timestamp":1743187051777,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"},"user_tz":240},"id":"2BKTChamRJHV","outputId":"b01e756d-df64-4b7f-e460-723515e02fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.057 ms\n","No errors!\n","Total Errors : 0\t"]}],"source":["!./block_queueing 25 64 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_64_64.raw \\\n","output_nextLevelNodes_bq_25_64_64.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_25_64_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_25_64_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"]},{"cell_type":"code","source":["!./block_queueing 35 32 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_32_32.raw \\\n","output_nextLevelNodes_bq_35_32_32.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_35_32_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_35_32_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKhlMjm3cO_Y","executionInfo":{"status":"ok","timestamp":1743187061501,"user_tz":240,"elapsed":9718,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"b35cf4a6-1e9b-48f9-bc9d-410f2915d913"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 3.822 ms\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","source":["!./block_queueing 35 32 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_32_64.raw \\\n","output_nextLevelNodes_bq_35_32_64.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_35_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_35_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmGMwnwFcTDU","executionInfo":{"status":"ok","timestamp":1743187070153,"user_tz":240,"elapsed":8640,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"4c8e0fee-0d4b-4eec-912e-ee6e81c306e2"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.395 ms\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","source":["!./block_queueing 35 64 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_64_32.raw \\\n","output_nextLevelNodes_bq_35_64_32.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_35_64_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_35_64_32.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZmhQIOGcfMu","executionInfo":{"status":"ok","timestamp":1743187079687,"user_tz":240,"elapsed":9533,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"58b21174-2b03-4ec9-ea9e-1f290b6d5c2d"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.190 ms\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","source":["!./block_queueing 35 64 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_64_64.raw \\\n","output_nextLevelNodes_bq_35_64_64.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_35_64_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_35_64_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3N-wzLH3cW2Y","executionInfo":{"status":"ok","timestamp":1743187089228,"user_tz":240,"elapsed":9532,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"23ad7144-8f0d-41d7-f3e1-6921d9ab6344"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Execution time: 4.217 ms\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 25 32 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_32_32.raw \\\n","output_nextLevelNodes_bq_25_32_32.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0FMA3GRk6oZ","executionInfo":{"status":"ok","timestamp":1743187091757,"user_tz":240,"elapsed":2527,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"087acab5-5056-4378-98c2-a9381342714f"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2321 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1368.458 ms\n","==PROF== Disconnected from process 2321\n","[2321] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (25, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.00\n","    SM Frequency                    Mhz       584.98\n","    Elapsed Cycles                cycle      949,891\n","    Memory Throughput                 %         1.42\n","    DRAM Throughput                   %         0.62\n","    Duration                         ms         1.62\n","    L1/TEX Cache Throughput           %         2.27\n","    L2 Cache Throughput               %         0.68\n","    SM Active Cycles              cycle   592,034.78\n","    Compute (SM) Throughput           %         1.42\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         2.23\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       40,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.07\n","    Executed Ipc Elapsed  inst/cycle         0.04\n","    Issue Slots Busy               %         1.65\n","    Issued Ipc Active     inst/cycle         0.07\n","    SM Busy                        %         1.65\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.98%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         1.97\n","    Mem Busy                    %         1.12\n","    Max Bandwidth               %         1.42\n","    L1/TEX Hit Rate             %        15.01\n","    L2 Hit Rate                 %        88.65\n","    Mem Pipes Busy              %         1.42\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 1.883%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.0 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.955%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         6.59\n","    Issued Warp Per Scheduler                        0.07\n","    No Eligible                            %        93.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.07\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 93.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 15.2 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        15.19\n","    Warp Cycles Per Executed Instruction           cycle        15.32\n","    Avg. Active Threads Per Warp                                 8.30\n","    Avg. Not Predicated Off Threads Per Warp                     7.97\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 49.81%                                                                                          \n","          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 49.8% of the total average of 15.2 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.063%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 8.3 threads being active per cycle. This is further reduced to  \n","          8.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch.          \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     9,667.27\n","    Executed Instructions                           inst    1,546,764\n","    Avg. Issued Instructions Per Scheduler          inst     9,748.02\n","    Issued Instructions                             inst    1,559,683\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             128\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread             800\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           40\n","    Block Limit Shared Mem                block          128\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.13\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.41%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM.                                                                                            \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    50,097.50\n","    Total DRAM Elapsed Cycles        cycle   64,919,552\n","    Average L1 Active Cycles         cycle   592,034.78\n","    Total L1 Elapsed Cycles          cycle   37,998,336\n","    Average L2 Active Cycles         cycle   281,592.31\n","    Total L2 Elapsed Cycles          cycle   44,425,632\n","    Average SM Active Cycles         cycle   592,034.78\n","    Total SM Elapsed Cycles          cycle   37,998,336\n","    Average SMSP Active Cycles       cycle   148,020.54\n","    Total SMSP Elapsed Cycles        cycle  151,993,344\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 23.42%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 37.58% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 13.15%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 84.40% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 23.42%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 37.58% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 9.857%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 48.60% above the average, while the minimum instance value is 8.51% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.21\n","    Branch Instructions              inst      321,701\n","    Branch Efficiency                   %        75.59\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 9.965%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (49% of the      \n","          total 275356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 25 32 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_32_64.raw \\\n","output_nextLevelNodes_bq_25_32_64.raw\n","\n","!./compareNextLevelNodes output_nextLevelNodes_bq_25_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nextLevelNodes.raw\n","!./compareNodeOutput output_nodeOutput_bq_25_32_64.raw /content/drive/MyDrive/ECSE420/lab3/sol_nodeOutput.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4_Wmws-lJBU","executionInfo":{"status":"ok","timestamp":1743187103396,"user_tz":240,"elapsed":11638,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"34967072-f962-40c3-bc25-b8f2eb1fee77"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2395 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1359.730 ms\n","==PROF== Disconnected from process 2395\n","[2395] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (25, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.00\n","    SM Frequency                    Mhz       584.97\n","    Elapsed Cycles                cycle      950,125\n","    Memory Throughput                 %         1.42\n","    DRAM Throughput                   %         0.62\n","    Duration                         ms         1.62\n","    L1/TEX Cache Throughput           %         2.27\n","    L2 Cache Throughput               %         0.68\n","    SM Active Cycles              cycle   592,374.97\n","    Compute (SM) Throughput           %         1.42\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         2.23\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       40,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.07\n","    Executed Ipc Elapsed  inst/cycle         0.04\n","    Issue Slots Busy               %         1.65\n","    Issued Ipc Active     inst/cycle         0.07\n","    SM Busy                        %         1.65\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.98%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         1.98\n","    Mem Busy                    %         1.12\n","    Max Bandwidth               %         1.42\n","    L1/TEX Hit Rate             %        15.00\n","    L2 Hit Rate                 %        88.65\n","    Mem Pipes Busy              %         1.42\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 1.883%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.0 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.955%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         6.59\n","    Issued Warp Per Scheduler                        0.07\n","    No Eligible                            %        93.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.07\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 93.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 15.2 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        15.19\n","    Warp Cycles Per Executed Instruction           cycle        15.31\n","    Avg. Active Threads Per Warp                                 8.30\n","    Avg. Not Predicated Off Threads Per Warp                     7.97\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 49.88%                                                                                          \n","          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 49.9% of the total average of 15.2 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.063%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 8.3 threads being active per cycle. This is further reduced to  \n","          8.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch.          \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     9,667.27\n","    Executed Instructions                           inst    1,546,764\n","    Avg. Issued Instructions Per Scheduler          inst     9,748.02\n","    Issued Instructions                             inst    1,559,683\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             256\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread             800\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           40\n","    Block Limit Shared Mem                block           64\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.12\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.41%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM.                                                                                            \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    50,161.50\n","    Total DRAM Elapsed Cycles        cycle   64,923,648\n","    Average L1 Active Cycles         cycle   592,374.97\n","    Total L1 Elapsed Cycles          cycle   37,993,632\n","    Average L2 Active Cycles         cycle   283,204.94\n","    Total L2 Elapsed Cycles          cycle   44,436,544\n","    Average SM Active Cycles         cycle   592,374.97\n","    Total SM Elapsed Cycles          cycle   37,993,632\n","    Average SMSP Active Cycles       cycle   148,017.89\n","    Total SMSP Elapsed Cycles        cycle  151,974,528\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 23.44%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 37.59% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 13.15%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 84.40% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 23.44%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 37.59% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 9.981%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 48.94% above the average, while the minimum instance value is 8.29% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.21\n","    Branch Instructions              inst      321,701\n","    Branch Efficiency                   %        75.59\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 10.02%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (49% of the      \n","          total 275356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n","No errors!\n","Total Errors : 0\t"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 25 64 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_64_32.raw \\\n","output_nextLevelNodes_bq_25_64_32.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgFr8mxclJG_","executionInfo":{"status":"ok","timestamp":1743187106048,"user_tz":240,"elapsed":2652,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"2983de4f-248c-42d8-a911-51330f3b41e3"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2509 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1458.159 ms\n","==PROF== Disconnected from process 2509\n","[2509] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (25, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.99\n","    SM Frequency                    Mhz       584.96\n","    Elapsed Cycles                cycle      812,715\n","    Memory Throughput                 %         1.96\n","    DRAM Throughput                   %         0.72\n","    Duration                         ms         1.39\n","    L1/TEX Cache Throughput           %         3.15\n","    L2 Cache Throughput               %         0.79\n","    SM Active Cycles              cycle   506,244.22\n","    Compute (SM) Throughput           %         1.96\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         2.23\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       40,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.09\n","    Executed Ipc Elapsed  inst/cycle         0.06\n","    Issue Slots Busy               %         2.30\n","    Issued Ipc Active     inst/cycle         0.09\n","    SM Busy                        %         2.30\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.59%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         2.30\n","    Mem Busy                    %         1.46\n","    Max Bandwidth               %         1.96\n","    L1/TEX Hit Rate             %        23.40\n","    L2 Hit Rate                 %        88.62\n","    Mem Pipes Busy              %         1.96\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 2.477%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.8 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 2.554%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.59\n","    Issued Warp Per Scheduler                        0.05\n","    No Eligible                            %        95.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.05\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 21.8 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        21.77\n","    Warp Cycles Per Executed Instruction           cycle        21.93\n","    Avg. Active Threads Per Warp                                12.15\n","    Avg. Not Predicated Off Threads Per Warp                    11.70\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 35.22%                                                                                          \n","          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for sibling warps at a CTA       \n","          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      \n","          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        \n","          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       \n","          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       \n","          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        \n","          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  \n","          first. This stall type represents about 35.2% of the total average of 21.8 cycles between issuing two         \n","          instructions.                                                                                                 \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 30.54%                                                                                          \n","          On average, each warp of this kernel spends 6.7 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 30.5% of the total average of 21.8 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.245%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 12.2 threads being active per cycle. This is further reduced    \n","          to 11.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst    11,544.93\n","    Executed Instructions                           inst    1,847,189\n","    Avg. Issued Instructions Per Scheduler          inst    11,626.30\n","    Issued Instructions                             inst    1,860,208\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             128\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,600\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           20\n","    Block Limit Shared Mem                block          128\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.25\n","    Achieved Active Warps Per SM           warp         2.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       49,924\n","    Total DRAM Elapsed Cycles        cycle   55,512,064\n","    Average L1 Active Cycles         cycle   506,244.22\n","    Total L1 Elapsed Cycles          cycle   32,521,656\n","    Average L2 Active Cycles         cycle   276,759.16\n","    Total L2 Elapsed Cycles          cycle   38,010,048\n","    Average SM Active Cycles         cycle   506,244.22\n","    Total SM Elapsed Cycles          cycle   32,521,656\n","    Average SMSP Active Cycles       cycle   253,035.87\n","    Total SMSP Elapsed Cycles        cycle  130,086,624\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 23.42%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 37.62% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 21.42%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 68.82% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 23.42%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 37.62% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11.09%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 47.60% above the average, while the minimum instance value is 8.62% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.22\n","    Branch Instructions              inst      401,701\n","    Branch Efficiency                   %        79.77\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 10.32%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (44% of the      \n","          total 305356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 25 64 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_25_64_64.raw \\\n","output_nextLevelNodes_bq_25_64_64.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2DN1u7UlJOM","executionInfo":{"status":"ok","timestamp":1743187108514,"user_tz":240,"elapsed":2450,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"197cc602-cf4a-417c-f655-97672f9c2c88"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2585 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1355.391 ms\n","==PROF== Disconnected from process 2585\n","[2585] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (25, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.99\n","    SM Frequency                    Mhz       584.97\n","    Elapsed Cycles                cycle      812,859\n","    Memory Throughput                 %         1.96\n","    DRAM Throughput                   %         0.72\n","    Duration                         ms         1.39\n","    L1/TEX Cache Throughput           %         3.15\n","    L2 Cache Throughput               %         0.79\n","    SM Active Cycles              cycle   506,255.40\n","    Compute (SM) Throughput           %         1.96\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         2.23\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       40,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.09\n","    Executed Ipc Elapsed  inst/cycle         0.06\n","    Issue Slots Busy               %         2.30\n","    Issued Ipc Active     inst/cycle         0.09\n","    SM Busy                        %         2.30\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.59%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         2.29\n","    Mem Busy                    %         1.46\n","    Max Bandwidth               %         1.96\n","    L1/TEX Hit Rate             %        23.39\n","    L2 Hit Rate                 %        88.52\n","    Mem Pipes Busy              %         1.96\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 2.478%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.8 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 2.555%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.59\n","    Issued Warp Per Scheduler                        0.05\n","    No Eligible                            %        95.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.05\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 21.8 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        21.76\n","    Warp Cycles Per Executed Instruction           cycle        21.92\n","    Avg. Active Threads Per Warp                                12.15\n","    Avg. Not Predicated Off Threads Per Warp                    11.70\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 35.24%                                                                                          \n","          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for sibling warps at a CTA       \n","          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      \n","          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        \n","          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       \n","          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       \n","          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        \n","          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  \n","          first. This stall type represents about 35.2% of the total average of 21.8 cycles between issuing two         \n","          instructions.                                                                                                 \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 30.57%                                                                                          \n","          On average, each warp of this kernel spends 6.7 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 30.6% of the total average of 21.8 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.245%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 12.2 threads being active per cycle. This is further reduced    \n","          to 11.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst    11,544.93\n","    Executed Instructions                           inst    1,847,189\n","    Avg. Issued Instructions Per Scheduler          inst    11,626.30\n","    Issued Instructions                             inst    1,860,208\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     25\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             256\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,600\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.04\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 37.5%                                                                                           \n","          The grid for this launch is configured to execute only 25 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           20\n","    Block Limit Shared Mem                block           64\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.25\n","    Achieved Active Warps Per SM           warp         2.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    49,727.50\n","    Total DRAM Elapsed Cycles        cycle   55,513,088\n","    Average L1 Active Cycles         cycle   506,255.40\n","    Total L1 Elapsed Cycles          cycle   32,511,200\n","    Average L2 Active Cycles         cycle   276,650.25\n","    Total L2 Elapsed Cycles          cycle   38,016,736\n","    Average SM Active Cycles         cycle   506,255.40\n","    Total SM Elapsed Cycles          cycle   32,511,200\n","    Average SMSP Active Cycles       cycle   253,078.64\n","    Total SMSP Elapsed Cycles        cycle  130,044,800\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 23.43%                                                                                          \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 37.61% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 21.43%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 68.81% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 23.43%                                                                                          \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 37.61% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11.06%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 47.50% above the average, while the minimum instance value is 7.92% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.22\n","    Branch Instructions              inst      401,701\n","    Branch Efficiency                   %        79.77\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 10.32%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (44% of the      \n","          total 305356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full /block_queueing 35 32 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_32_32.raw \\\n","output_nextLevelNodes_bq_35_32_32.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRoOQ8vGlJSt","executionInfo":{"status":"ok","timestamp":1743187110902,"user_tz":240,"elapsed":2387,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"aca3a23d-182c-4d29-83cf-e404e9e0537c"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2655 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1335.010 ms\n","==PROF== Disconnected from process 2655\n","[2655] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (35, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.99\n","    SM Frequency                    Mhz       584.97\n","    Elapsed Cycles                cycle      680,252\n","    Memory Throughput                 %         1.98\n","    DRAM Throughput                   %         0.86\n","    Duration                         ms         1.16\n","    L1/TEX Cache Throughput           %         3.12\n","    L2 Cache Throughput               %         0.95\n","    SM Active Cycles              cycle   593,239.35\n","    Compute (SM) Throughput           %         1.98\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         4.19\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.07\n","    Executed Ipc Elapsed  inst/cycle         0.06\n","    Issue Slots Busy               %         1.64\n","    Issued Ipc Active     inst/cycle         0.07\n","    SM Busy                        %         1.64\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.99%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         2.75\n","    Mem Busy                    %         1.56\n","    Max Bandwidth               %         1.98\n","    L1/TEX Hit Rate             %        15.05\n","    L2 Hit Rate                 %        88.29\n","    Mem Pipes Busy              %         1.98\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 2.629%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.0 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 2.73%                                                                                           \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         6.57\n","    Issued Warp Per Scheduler                        0.07\n","    No Eligible                            %        93.43\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.07\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 93.43%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 15.2 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        15.22\n","    Warp Cycles Per Executed Instruction           cycle        15.34\n","    Avg. Active Threads Per Warp                                 8.30\n","    Avg. Not Predicated Off Threads Per Warp                     7.97\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 49.94%                                                                                          \n","          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 49.9% of the total average of 15.2 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.485%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 8.3 threads being active per cycle. This is further reduced to  \n","          8.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch.          \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     9,668.34\n","    Executed Instructions                           inst    1,546,934\n","    Avg. Issued Instructions Per Scheduler          inst     9,749.33\n","    Issued Instructions                             inst    1,559,893\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             128\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,120\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           40\n","    Block Limit Shared Mem                block          128\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.13\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.43%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM.                                                                                            \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    50,042.50\n","    Total DRAM Elapsed Cycles        cycle   46,465,024\n","    Average L1 Active Cycles         cycle   593,239.35\n","    Total L1 Elapsed Cycles          cycle   27,214,472\n","    Average L2 Active Cycles         cycle   269,700.53\n","    Total L2 Elapsed Cycles          cycle   31,814,848\n","    Average SM Active Cycles         cycle   593,239.35\n","    Total SM Elapsed Cycles          cycle   27,214,472\n","    Average SMSP Active Cycles       cycle   148,317.28\n","    Total SMSP Elapsed Cycles        cycle  108,857,888\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 12.61% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 17.04%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 78.15% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 12.61% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 12.78%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 47.11% above the average, while the minimum instance value is 6.56% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.21\n","    Branch Instructions              inst      321,701\n","    Branch Efficiency                   %        75.59\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 13.33%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (49% of the      \n","          total 275356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 35 32 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_32_64.raw \\\n","output_nextLevelNodes_bq_35_32_64.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXnkZmmnlJY1","executionInfo":{"status":"ok","timestamp":1743187114424,"user_tz":240,"elapsed":3521,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"dcdda764-d558-4015-9aba-eb4dc55095fb"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2731 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1756.641 ms\n","==PROF== Disconnected from process 2731\n","[2731] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (35, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.00\n","    SM Frequency                    Mhz       584.96\n","    Elapsed Cycles                cycle      680,389\n","    Memory Throughput                 %         1.98\n","    DRAM Throughput                   %         0.86\n","    Duration                         ms         1.16\n","    L1/TEX Cache Throughput           %         3.12\n","    L2 Cache Throughput               %         0.94\n","    SM Active Cycles              cycle   593,291.80\n","    Compute (SM) Throughput           %         1.98\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         4.19\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.07\n","    Executed Ipc Elapsed  inst/cycle         0.06\n","    Issue Slots Busy               %         1.64\n","    Issued Ipc Active     inst/cycle         0.07\n","    SM Busy                        %         1.64\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.99%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         2.74\n","    Mem Busy                    %         1.56\n","    Max Bandwidth               %         1.98\n","    L1/TEX Hit Rate             %        15.04\n","    L2 Hit Rate                 %        88.89\n","    Mem Pipes Busy              %         1.98\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 2.63%                                                                                           \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 5.0 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 2.731%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         6.57\n","    Issued Warp Per Scheduler                        0.07\n","    No Eligible                            %        93.43\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.07\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 93.43%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 15.2 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        15.22\n","    Warp Cycles Per Executed Instruction           cycle        15.34\n","    Avg. Active Threads Per Warp                                 8.30\n","    Avg. Not Predicated Off Threads Per Warp                     7.97\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 49.92%                                                                                          \n","          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 49.9% of the total average of 15.2 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.485%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 8.3 threads being active per cycle. This is further reduced to  \n","          8.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch.          \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst     9,668.34\n","    Executed Instructions                           inst    1,546,934\n","    Avg. Issued Instructions Per Scheduler          inst     9,749.33\n","    Issued Instructions                             inst    1,559,893\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    32\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             256\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           1,120\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           40\n","    Block Limit Shared Mem                block           64\n","    Block Limit Warps                     block           32\n","    Theoretical Active Warps per SM        warp           16\n","    Theoretical Occupancy                     %           50\n","    Achieved Occupancy                        %         3.13\n","    Achieved Active Warps Per SM           warp         1.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.43%                                                                                          \n","          The difference between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the       \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 50%                                                                                             \n","          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n","          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n","          can fit on the SM.                                                                                            \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle    49,878.50\n","    Total DRAM Elapsed Cycles        cycle   46,479,360\n","    Average L1 Active Cycles         cycle   593,291.80\n","    Total L1 Elapsed Cycles          cycle   27,205,536\n","    Average L2 Active Cycles         cycle   267,847.47\n","    Total L2 Elapsed Cycles          cycle   31,821,248\n","    Average SM Active Cycles         cycle   593,291.80\n","    Total SM Elapsed Cycles          cycle   27,205,536\n","    Average SMSP Active Cycles       cycle   148,331.99\n","    Total SMSP Elapsed Cycles        cycle  108,822,144\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 12.61% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 17.04%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 78.16% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 12.61% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 12.72%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 47.22% above the average, while the minimum instance value is 5.37% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.21\n","    Branch Instructions              inst      321,701\n","    Branch Efficiency                   %        75.59\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 13.23%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (49% of the      \n","          total 275356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 35 64 32 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_64_32.raw \\\n","output_nextLevelNodes_bq_35_64_32.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYkFoCBGlJgM","executionInfo":{"status":"ok","timestamp":1743187116808,"user_tz":240,"elapsed":2381,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"dc9f026b-b6bf-4666-ee49-e6ba37012395"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2805 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1290.202 ms\n","==PROF== Disconnected from process 2805\n","[2805] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (35, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         5.00\n","    SM Frequency                    Mhz       584.95\n","    Elapsed Cycles                cycle      581,503\n","    Memory Throughput                 %         2.74\n","    DRAM Throughput                   %         1.01\n","    Duration                         us       994.11\n","    L1/TEX Cache Throughput           %         4.08\n","    L2 Cache Throughput               %         1.11\n","    SM Active Cycles              cycle   507,261.85\n","    Compute (SM) Throughput           %         2.74\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         4.19\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.09\n","    Executed Ipc Elapsed  inst/cycle         0.08\n","    Issue Slots Busy               %         2.29\n","    Issued Ipc Active     inst/cycle         0.09\n","    SM Busy                        %         2.29\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.59%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         3.23\n","    Mem Busy                    %         2.04\n","    Max Bandwidth               %         2.74\n","    L1/TEX Hit Rate             %        23.34\n","    L2 Hit Rate                 %        88.55\n","    Mem Pipes Busy              %         2.74\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 3.461%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.8 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 3.569%                                                                                          \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.59\n","    Issued Warp Per Scheduler                        0.05\n","    No Eligible                            %        95.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.05\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 21.8 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        21.79\n","    Warp Cycles Per Executed Instruction           cycle        21.95\n","    Avg. Active Threads Per Warp                                12.16\n","    Avg. Not Predicated Off Threads Per Warp                    11.70\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 35.25%                                                                                          \n","          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for sibling warps at a CTA       \n","          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      \n","          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        \n","          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       \n","          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       \n","          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        \n","          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  \n","          first. This stall type represents about 35.3% of the total average of 21.8 cycles between issuing two         \n","          instructions.                                                                                                 \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 30.58%                                                                                          \n","          On average, each warp of this kernel spends 6.7 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 30.6% of the total average of 21.8 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.739%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 12.2 threads being active per cycle. This is further reduced    \n","          to 11.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst    11,547.06\n","    Executed Instructions                           inst    1,847,529\n","    Avg. Issued Instructions Per Scheduler          inst    11,628.92\n","    Issued Instructions                             inst    1,860,628\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             128\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           2,240\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           20\n","    Block Limit Shared Mem                block          128\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.25\n","    Achieved Active Warps Per SM           warp         2.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       50,097\n","    Total DRAM Elapsed Cycles        cycle   39,739,392\n","    Average L1 Active Cycles         cycle   507,261.85\n","    Total L1 Elapsed Cycles          cycle   23,271,808\n","    Average L2 Active Cycles         cycle   261,261.72\n","    Total L2 Elapsed Cycles          cycle   27,196,416\n","    Average SM Active Cycles         cycle   507,261.85\n","    Total SM Elapsed Cycles          cycle   23,271,808\n","    Average SMSP Active Cycles       cycle   253,518.70\n","    Total SMSP Elapsed Cycles        cycle   93,087,232\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 12.62% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.54%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 56.32% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 12.62% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 14.09%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 45.85% above the average, while the minimum instance value is 5.61% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.22\n","    Branch Instructions              inst      401,701\n","    Branch Efficiency                   %        79.76\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 13.62%                                                                                          \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (44% of the      \n","          total 305356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]},{"cell_type":"code","source":["!ncu --set full ./block_queueing 35 64 64 \\\n","/content/drive/MyDrive/ECSE420/lab3/input1.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input2.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input3.raw \\\n","/content/drive/MyDrive/ECSE420/lab3/input4.raw \\\n","output_nodeOutput_bq_35_64_64.raw \\\n","output_nextLevelNodes_bq_35_64_64.raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Cct_vfAlJnr","executionInfo":{"status":"ok","timestamp":1743187119266,"user_tz":240,"elapsed":2457,"user":{"displayName":"William Zimmerman","userId":"06011454881744315213"}},"outputId":"aa18c789-61a3-450d-d119-c3ffe5a56d0e"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["==PROF== Connected to process 2875 (/content/block_queueing)\n","==PROF== Profiling \"block_queueing_kernel\" - 0: 0%....50%....100% - 30 passes\n","Execution time: 1312.479 ms\n","==PROF== Disconnected from process 2875\n","[2875] block_queueing@127.0.0.1\n","  block_queueing_kernel(int *, int *, int *, int *, int *, int *, int *, int, int *, int *, int) (35, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: GPU Speed Of Light Throughput\n","    ----------------------- ----------- ------------\n","    Metric Name             Metric Unit Metric Value\n","    ----------------------- ----------- ------------\n","    DRAM Frequency                  Ghz         4.99\n","    SM Frequency                    Mhz       584.95\n","    Elapsed Cycles                cycle      581,790\n","    Memory Throughput                 %         2.74\n","    DRAM Throughput                   %         1.02\n","    Duration                         us       994.59\n","    L1/TEX Cache Throughput           %         4.08\n","    L2 Cache Throughput               %         1.10\n","    SM Active Cycles              cycle   507,042.35\n","    Compute (SM) Throughput           %         2.74\n","    ----------------------- ----------- ------------\n","\n","    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n","          waves across all SMs. Look at Launch Statistics for more details.                                             \n","\n","    Section: GPU Speed Of Light Roofline Chart\n","    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n","          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n","          analysis.                                                                                                     \n","\n","    Section: PM Sampling\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Maximum Buffer Size             Mbyte         4.19\n","    Dropped Samples                sample            0\n","    Maximum Sampling Interval       cycle       20,000\n","    # Pass Groups                                    1\n","    ------------------------- ----------- ------------\n","\n","    Section: Compute Workload Analysis\n","    -------------------- ----------- ------------\n","    Metric Name          Metric Unit Metric Value\n","    -------------------- ----------- ------------\n","    Executed Ipc Active   inst/cycle         0.09\n","    Executed Ipc Elapsed  inst/cycle         0.08\n","    Issue Slots Busy               %         2.29\n","    Issued Ipc Active     inst/cycle         0.09\n","    SM Busy                        %         2.29\n","    -------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 98.59%                                                                                    \n","          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n","          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n","\n","    Section: Memory Workload Analysis\n","    ----------------- ----------- ------------\n","    Metric Name       Metric Unit Metric Value\n","    ----------------- ----------- ------------\n","    Memory Throughput     Gbyte/s         3.25\n","    Mem Busy                    %         2.04\n","    Max Bandwidth               %         2.74\n","    L1/TEX Hit Rate             %        23.35\n","    L2 Hit Rate                 %        88.09\n","    Mem Pipes Busy              %         2.74\n","    ----------------- ----------- ------------\n","\n","    Section: Memory Workload Analysis Chart\n","    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n","          additional metric could enable the rule to provide more guidance.                                             \n","\n","    Section: Memory Workload Analysis Tables\n","    OPT   Est. Speedup: 3.462%                                                                                          \n","          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.8 of the 32    \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global loads.                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 3.57%                                                                                           \n","          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     \n","          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   \n","          threads. Check the Source Counters section for uncoalesced global stores.                                     \n","\n","    Section: Scheduler Statistics\n","    ---------------------------- ----------- ------------\n","    Metric Name                  Metric Unit Metric Value\n","    ---------------------------- ----------- ------------\n","    One or More Eligible                   %         4.59\n","    Issued Warp Per Scheduler                        0.05\n","    No Eligible                            %        95.41\n","    Active Warps Per Scheduler          warp         1.00\n","    Eligible Warps Per Scheduler        warp         0.05\n","    ---------------------------- ----------- ------------\n","\n","    OPT   Est. Local Speedup: 95.41%                                                                                    \n","          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n","          issues an instruction every 21.8 cycles. This might leave hardware resources underutilized and may lead to    \n","          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n","          1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    \n","          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n","          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n","          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n","          State Statistics and Source Counters sections.                                                                \n","\n","    Section: Warp State Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Warp Cycles Per Issued Instruction             cycle        21.79\n","    Warp Cycles Per Executed Instruction           cycle        21.95\n","    Avg. Active Threads Per Warp                                12.16\n","    Avg. Not Predicated Off Threads Per Warp                    11.70\n","    ---------------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 35.26%                                                                                          \n","          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for sibling warps at a CTA       \n","          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      \n","          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        \n","          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       \n","          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       \n","          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        \n","          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  \n","          first. This stall type represents about 35.3% of the total average of 21.8 cycles between issuing two         \n","          instructions.                                                                                                 \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 30.6%                                                                                           \n","          On average, each warp of this kernel spends 6.7 cycles being stalled waiting for a scoreboard dependency on a \n","          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n","          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n","          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n","          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n","          shared memory. This stall type represents about 30.6% of the total average of 21.8 cycles between issuing     \n","          two instructions.                                                                                             \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n","          sampling data. The Kernel Profiling Guide                                                                     \n","          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n","          on each stall reason.                                                                                         \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 1.739%                                                                                          \n","          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n","          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n","          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n","          per cycle. This kernel achieves an average of 12.2 threads being active per cycle. This is further reduced    \n","          to 11.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n","          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n","          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n","\n","    Section: Instruction Statistics\n","    ---------------------------------------- ----------- ------------\n","    Metric Name                              Metric Unit Metric Value\n","    ---------------------------------------- ----------- ------------\n","    Avg. Executed Instructions Per Scheduler        inst    11,547.06\n","    Executed Instructions                           inst    1,847,529\n","    Avg. Issued Instructions Per Scheduler          inst    11,628.92\n","    Issued Instructions                             inst    1,860,628\n","    ---------------------------------------- ----------- ------------\n","\n","    Section: Launch Statistics\n","    -------------------------------- --------------- ---------------\n","    Metric Name                          Metric Unit    Metric Value\n","    -------------------------------- --------------- ---------------\n","    Block Size                                                    64\n","    Function Cache Configuration                     CachePreferNone\n","    Grid Size                                                     35\n","    Registers Per Thread             register/thread              44\n","    Shared Memory Configuration Size           Kbyte           32.77\n","    Driver Shared Memory Per Block        byte/block               0\n","    Dynamic Shared Memory Per Block       byte/block             256\n","    Static Shared Memory Per Block        byte/block              16\n","    # SMs                                         SM              40\n","    Threads                                   thread           2,240\n","    Uses Green Context                                             0\n","    Waves Per SM                                                0.05\n","    -------------------------------- --------------- ---------------\n","\n","    OPT   Est. Speedup: 12.5%                                                                                           \n","          The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 40             \n","          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n","          concurrently with other workloads, consider reducing the block size to have at least one block per            \n","          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n","          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n","          description for more details on launch configurations.                                                        \n","\n","    Section: Occupancy\n","    ------------------------------- ----------- ------------\n","    Metric Name                     Metric Unit Metric Value\n","    ------------------------------- ----------- ------------\n","    Block Limit SM                        block           16\n","    Block Limit Registers                 block           20\n","    Block Limit Shared Mem                block           64\n","    Block Limit Warps                     block           16\n","    Theoretical Active Warps per SM        warp           32\n","    Theoretical Occupancy                     %          100\n","    Achieved Occupancy                        %         6.25\n","    Achieved Active Warps Per SM           warp         2.00\n","    ------------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 93.75%                                                                                          \n","          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      \n","          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n","          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n","          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n","          optimizing occupancy.                                                                                         \n","\n","    Section: GPU and Memory Workload Distribution\n","    -------------------------- ----------- ------------\n","    Metric Name                Metric Unit Metric Value\n","    -------------------------- ----------- ------------\n","    Average DRAM Active Cycles       cycle       50,577\n","    Total DRAM Elapsed Cycles        cycle   39,735,296\n","    Average L1 Active Cycles         cycle   507,042.35\n","    Total L1 Elapsed Cycles          cycle   23,269,072\n","    Average L2 Active Cycles         cycle   262,988.81\n","    Total L2 Elapsed Cycles          cycle   27,209,856\n","    Average SM Active Cycles         cycle   507,042.35\n","    Total SM Elapsed Cycles          cycle   23,269,072\n","    Average SMSP Active Cycles       cycle   253,415.77\n","    Total SMSP Elapsed Cycles        cycle   93,076,288\n","    -------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n","          instance value is 12.62% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 24.54%                                                                                          \n","          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n","          instance value is 56.33% above the average, while the minimum instance value is 100.00% below the average.    \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 11%                                                                                             \n","          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n","          Maximum instance value is 12.62% above the average, while the minimum instance value is 100.00% below the     \n","          average.                                                                                                      \n","    ----- --------------------------------------------------------------------------------------------------------------\n","    OPT   Est. Speedup: 14.31%                                                                                          \n","          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n","          Maximum instance value is 46.27% above the average, while the minimum instance value is 5.68% below the       \n","          average.                                                                                                      \n","\n","    Section: Source Counters\n","    ------------------------- ----------- ------------\n","    Metric Name               Metric Unit Metric Value\n","    ------------------------- ----------- ------------\n","    Branch Instructions Ratio           %         0.22\n","    Branch Instructions              inst      401,701\n","    Branch Efficiency                   %        79.76\n","    Avg. Divergent Branches                     295.81\n","    ------------------------- ----------- ------------\n","\n","    OPT   Est. Speedup: 13.7%                                                                                           \n","          This kernel has uncoalesced global accesses resulting in a total of 135285 excessive sectors (44% of the      \n","          total 305356 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         \n","          locations. The CUDA Programming Guide                                                                         \n","          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      \n","          information on reducing uncoalesced device memory accesses.                                                   \n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OQ-8T1zfKZbv"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}