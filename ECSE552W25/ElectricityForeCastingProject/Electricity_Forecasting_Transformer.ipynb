{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedexLI1oAVr",
        "outputId": "6ac0e0ff-d09e-40bd-ad64-6257a97ebe29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.570.86)\n",
            "Collecting zeus[bso]\n",
            "  Downloading zeus-0.11.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (1.6.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (12.570.86)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (2.11.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (13.9.4)\n",
            "Collecting tyro (from zeus[bso])\n",
            "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (0.28.1)\n",
            "Collecting amdsmi (from zeus[bso])\n",
            "  Downloading amdsmi-6.4.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from zeus[bso]) (2.8.2)\n",
            "Collecting pydantic (from zeus[bso])\n",
            "  Downloading pydantic-1.10.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus[bso]) (4.13.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->zeus[bso]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->zeus[bso]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->zeus[bso]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->zeus[bso]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->zeus[bso]) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus[bso]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus[bso]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->zeus[bso]) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus[bso]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus[bso]) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->zeus[bso]) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->zeus[bso]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->zeus[bso]) (3.6.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus[bso]) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->zeus[bso])\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus[bso]) (4.4.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zeus[bso]) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->zeus[bso]) (1.3.1)\n",
            "Downloading pydantic-1.10.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading amdsmi-6.4.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zeus-0.11.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, pydantic, amdsmi, tyro, zeus\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.2\n",
            "    Uninstalling pydantic-2.11.2:\n",
            "      Successfully uninstalled pydantic-2.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.23 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.21 which is incompatible.\n",
            "albumentations 2.0.5 requires pydantic>=2.9.2, but you have pydantic 1.10.21 which is incompatible.\n",
            "thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.21 which is incompatible.\n",
            "google-genai 1.9.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.21 which is incompatible.\n",
            "langchain-core 0.3.51 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.21 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed amdsmi-6.4.0 pydantic-1.10.21 shtab-1.7.2 tyro-0.9.18 zeus-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml\n",
        "\n",
        "!pip install zeus[bso]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ABGZYio5F1",
        "outputId": "8de1fba8-67f4-4349-bea1-10086d9fc196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/opt/rocm/lib/libamd_smi.so: cannot open shared object file: No such file or directory\n",
            "Unable to find libamd_smi.so library try installing amd-smi-lib from your package manager\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pynvml import *\n",
        "#from codecarbon import EmissionsTracker\n",
        "import torch.profiler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import time\n",
        "from zeus.optimizer.power_limit import GlobalPowerLimitOptimizer\n",
        "from zeus.monitor import ZeusMonitor\n",
        "from zeus.optimizer.batch_size import BatchSizeOptimizer, JobSpec\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import random\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn24111sPuNO",
        "outputId": "50fa267d-6fa1-4b57-ee10-56f69eea6782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TORCH_MHA_DISABLE_EFFICIENT\"] = \"1\"  # Force fallback to standard attention kernel\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#tracker.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUsP-_wi1lP8"
      },
      "outputs": [],
      "source": [
        "batch_opt = False\n",
        "optimize = False\n",
        "freq_opt = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZsxyWnHpbMF"
      },
      "outputs": [],
      "source": [
        "# def create_sliding_windows(df, input_window=168, forecast_horizon=24, target_column='Settlement Point Price'):\n",
        "#     X, y = [], []\n",
        "#     df = df.sort_index()\n",
        "#     for i in range(len(df) - input_window - forecast_horizon + 1):\n",
        "#         X_window = df.iloc[i:i+input_window].drop(target_column, axis=1).values\n",
        "#         y_window = df[target_column].iloc[i+input_window:i+input_window+forecast_horizon].values\n",
        "#         X.append(X_window)\n",
        "#         y.append(y_window)\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "\n",
        "def create_sliding_windows_with_future_weather(df, input_window=24, forecast_horizon=24,\n",
        "                                               target_column='Settlement Point Price',\n",
        "                                               weather_columns=['Air Temp (°C)', 'Wind Speed (m/s)']):\n",
        "\n",
        "    X, y = [], []\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Get the list (Index) of feature columns (all columns except target)\n",
        "    full_feature_columns = df.columns.drop(target_column)\n",
        "    n_features = len(full_feature_columns)\n",
        "\n",
        "    for i in range(len(df) - input_window - forecast_horizon + 1):\n",
        "        # Historical window: use all features (except target)\n",
        "        X_hist = df.iloc[i : i + input_window].drop(target_column, axis=1).values  # shape (input_window, n_features)\n",
        "\n",
        "        # Future window: create an array of zeros with the same number of features\n",
        "        X_future_full = np.zeros((forecast_horizon, n_features))\n",
        "\n",
        "        # Get the forecast weather values and fill into the corresponding columns\n",
        "        future_df = df.iloc[i + input_window : i + input_window + forecast_horizon]\n",
        "        for col in weather_columns:\n",
        "            if col in full_feature_columns:\n",
        "                # Find the index of the weather column in the feature set\n",
        "                idx = full_feature_columns.get_loc(col)\n",
        "                X_future_full[:, idx] = future_df[col].values\n",
        "            else:\n",
        "                raise ValueError(f\"Column '{col}' not found in dataframe columns: {list(full_feature_columns)}\")\n",
        "\n",
        "        # Concatenate along the time axis to form the combined input window\n",
        "        X_combined = np.concatenate([X_hist, X_future_full], axis=0)\n",
        "\n",
        "        # The target is the forecast horizon values for the target column.\n",
        "        y_window = df[target_column].iloc[i + input_window : i + input_window + forecast_horizon].values\n",
        "\n",
        "        X.append(X_combined)\n",
        "        y.append(y_window)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKK-GnJGsh8q"
      },
      "outputs": [],
      "source": [
        "#inspo: https://stackoverflow.com/questions/77444485/using-positional-encoding-in-pytorch\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncodingExo(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix once\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
        "        self.register_buffer('pe', pe)  # makes it non-trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (seq_len, batch_size, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerExogenousForecastModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, seq_len, forecast_horizon, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        # Project raw features to d_model\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncodingExo(d_model, dropout, max_len=seq_len)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=4 * d_model, dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Final output head\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, forecast_horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, forecast_horizon)\n",
        "        \"\"\"\n",
        "        x = self.input_proj(x)           # (B, T, d_model)\n",
        "        x = x.transpose(0, 1)            # (T, B, d_model)\n",
        "        x = self.pos_encoder(x)          # (T, B, d_model)\n",
        "        x = self.transformer_encoder(x)  # (T, B, d_model)\n",
        "        x = x.transpose(0, 1)            # (B, T, d_model)\n",
        "\n",
        "        # Mean pool over time\n",
        "        pooled = x.mean(dim=1)           # (B, d_model)\n",
        "        return self.fc_out(pooled)       # (B, forecast_horizon)\n",
        "\n",
        "class TransformerSeq2SeqForecastModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                 seq_len, forecast_horizon, dropout=0.1):\n",
        "        super().__init__()\n",
        "        dim_feedforward = 4 * d_model\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.tgt_proj = nn.Linear(1, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=seq_len)\n",
        "        self.pos_decoder = PositionalEncoding(d_model, dropout, max_len=forecast_horizon)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "        self.fc_out = nn.Linear(d_model, 1)\n",
        "        self.seq_len = seq_len\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "    def forward(self, src_inputs, tgt=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src_inputs: A list containing:\n",
        "                - src_inputs[0]: historical sequence Tensor of shape (B, seq_len, input_dim)\n",
        "                - src_inputs[1]: future time encoding Tensor.\n",
        "            tgt: Optional target Tensor for teacher forcing.\n",
        "        Returns:\n",
        "            Tensor of shape (B, forecast_horizon) with forecasted values.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # === ENCODER ===\n",
        "        src_emb = self.input_proj(src)  # (B, seq_len, d_model)\n",
        "        src_emb = self.pos_encoder(src_emb.transpose(0, 1))  # (seq_len, B, d_model)\n",
        "        memory = self.transformer_encoder(src_emb)  # (seq_len, B, d_model)\n",
        "\n",
        "        # === DECODER ===\n",
        "        if tgt is not None:\n",
        "            # Teacher forcing mode:\n",
        "            if tgt.dim() == 2:\n",
        "                tgt = tgt.unsqueeze(-1)  # (B, H, 1)\n",
        "            decoder_input = tgt[:, :-1, :]  # (B, H-1, input_dim)\n",
        "            start_token = torch.zeros(batch_size, 1, tgt.size(-1), device=tgt.device)\n",
        "            decoder_input = torch.cat([start_token, decoder_input], dim=1)  # (B, H, input_dim)\n",
        "            decoder_input = self.tgt_proj(decoder_input)  # (B, H, d_model)\n",
        "            decoder_input = decoder_input.transpose(0, 1)  # (H, B, d_model)\n",
        "            decoder_input = self.pos_decoder(decoder_input)\n",
        "            tgt_len = decoder_input.size(0)\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
        "            decoder_output = self.transformer_decoder(decoder_input, memory, tgt_mask=tgt_mask)\n",
        "            output = self.fc_out(decoder_output)  # (H, B, 1)\n",
        "            output = output.transpose(0, 1).squeeze(-1)  # (B, H)\n",
        "        else:\n",
        "            # Autoregressive inference (no teacher forcing)\n",
        "            outputs = []\n",
        "            prev_y = torch.zeros(batch_size, 1, device=src.device)\n",
        "            for t in range(self.forecast_horizon):\n",
        "                dec_input_step = self.tgt_proj(prev_y.unsqueeze(-1))  # (B, 1, d_model)\n",
        "                dec_input_step = dec_input_step.transpose(0, 1)  # (1, B, d_model)\n",
        "                dec_input_step = self.pos_decoder(dec_input_step)\n",
        "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(dec_input_step.size(0)).to(src.device)\n",
        "                dec_output = self.transformer_decoder(dec_input_step, memory, tgt_mask=tgt_mask)\n",
        "                out_step = self.fc_out(dec_output)  # (1, B, 1)\n",
        "                out_step = out_step.squeeze(0).squeeze(-1)  # (B,)\n",
        "                outputs.append(out_step)\n",
        "                prev_y = out_step.unsqueeze(1)\n",
        "            output = torch.stack(outputs, dim=1)  # (B, H)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtAw4aAuyKRw"
      },
      "outputs": [],
      "source": [
        "# config = TimeSeriesTransformerConfig(\n",
        "#     feature_size=11,            # number of features per timestep\n",
        "#     hidden_size=128,            # embedding dimension for the transformer\n",
        "#     num_hidden_layers=4,        # number of encoder layers\n",
        "#     num_attention_heads=8,      # number of attention heads\n",
        "#     dropout=0.1,                # dropout probability\n",
        "#     layer_norm_eps=1e-5,        # layer norm epsilon\n",
        "#     initializer_range=0.02,     # weight initialization range\n",
        "# )\n",
        "\n",
        "# model = TimeSeriesTransformerModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjzcVl9EpbSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('ERCOT_Data/Processed/Houston_Load_SPP_Weather_Combined_updated.csv')\n",
        "df_unscaled = pd.read_csv('ERCOT_Data/Processed/Houston_Load_SPP_Weather_Combined_updated.csv')\n",
        "\n",
        "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "df = df.sort_values('Datetime').reset_index(drop=True)\n",
        "\n",
        "# Extract hour,day,month features\n",
        "df['hour'] = df['Datetime'].dt.hour\n",
        "df['dayofweek'] = df['Datetime'].dt.dayofweek\n",
        "df['month'] = df['Datetime'].dt.month\n",
        "\n",
        "# Cyclic encoding\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "# imputing NAN vals (i figure it's fine because there are so few)\n",
        "df['Air Temp (°C)'] = df['Air Temp (°C)'].interpolate(method='linear').ffill().bfill()\n",
        "df['Wind Speed (m/s)'] = df['Wind Speed (m/s)'].interpolate(method='linear').ffill().bfill()\n",
        "\n",
        "\n",
        "\n",
        "#defing features and target\n",
        "\n",
        "feature_cols = [\n",
        "    'Settlement Point Price',\n",
        "    #'Houston_Load',\n",
        "    'Wind Speed (m/s)', 'Air Temp (°C)',\n",
        "    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos'\n",
        "]\n",
        "time_encoding_cols = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
        "\n",
        "y_col = 'Settlement Point Price'\n",
        "\n",
        "\n",
        "# Parameters\n",
        "lookback_window = 24\n",
        "forecast_horizon = 24\n",
        "\n",
        "# Define test set as last 5%\n",
        "test_size = int(0.10 * len(df))\n",
        "test_cutoff_index = len(df) - test_size\n",
        "df_trainval = df.iloc[:test_cutoff_index].copy()\n",
        "df_test = df.iloc[test_cutoff_index - lookback_window:].copy()  # ensure overlap for lookback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjAG-5f-AOye",
        "outputId": "43aaa1ef-a31b-4e5b-e1b8-80a83c35f168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hour range: 0 → 23\n",
            "             Datetime\n",
            "0 2021-04-15 13:00:00\n",
            "1 2021-04-15 14:00:00\n",
            "2 2021-04-15 15:00:00\n",
            "3 2021-04-15 16:00:00\n",
            "4 2021-04-15 17:00:00\n",
            "                 Datetime\n",
            "29264 2024-12-31 19:00:00\n",
            "29265 2024-12-31 20:00:00\n",
            "29266 2024-12-31 21:00:00\n",
            "29267 2024-12-31 22:00:00\n",
            "29268 2024-12-31 23:00:00\n"
          ]
        }
      ],
      "source": [
        "print(\"Hour range:\", df['hour'].min(), \"→\", df['hour'].max())\n",
        "\n",
        "print(df[['Datetime']].head())\n",
        "print(df[['Datetime']].tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "129lzlhxATtX"
      },
      "outputs": [],
      "source": [
        "# Scale\n",
        "scaler_target = StandardScaler()\n",
        "scaler_weather = StandardScaler()\n",
        "scaler_target.fit(df_trainval[[y_col]])\n",
        "scaler_weather.fit(df_trainval[['Air Temp (°C)', 'Wind Speed (m/s)']])\n",
        "\n",
        "df_trainval[y_col] = scaler_target.transform(df_trainval[[y_col]])\n",
        "df_trainval[['Air Temp (°C)', 'Wind Speed (m/s)']] = scaler_weather.transform(\n",
        "df_trainval[['Air Temp (°C)', 'Wind Speed (m/s)']])\n",
        "\n",
        "df_test_scaled = df_test.copy()\n",
        "df_test_scaled[y_col] = scaler_target.transform(df_test_scaled[[y_col]])\n",
        "df_test_scaled[['Air Temp (°C)', 'Wind Speed (m/s)']] = scaler_weather.transform(\n",
        "df_test_scaled[['Air Temp (°C)', 'Wind Speed (m/s)']])\n",
        "\n",
        "# Sliding window function\n",
        "def create_sliding_windows_with_future_time(df, lookback, horizon, feature_cols, time_encoding_cols, target_col):\n",
        "    x_list, time_future_list, y_list = [], [], []\n",
        "    data = df[feature_cols].values\n",
        "    time_enc = df[time_encoding_cols].values\n",
        "    target = df[[target_col]].values\n",
        "    for t in range(lookback, len(df) - horizon):\n",
        "        x_seq = data[t - lookback:t]\n",
        "        future_time = time_enc[t:t + horizon]\n",
        "        y = target[t:t + horizon].flatten()\n",
        "        x_list.append(x_seq)\n",
        "        time_future_list.append(future_time)\n",
        "        y_list.append(y)\n",
        "    return np.array(x_list), np.array(time_future_list), np.array(y_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTn-8aWDwENj",
        "outputId": "ad51dd13-5ff1-4071-f17c-4a611b4a2f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_seq_train shape: (26295, 24, 9)\n",
            "time_enc_train shape: (26295, 24, 6)\n",
            "y_train shape: (26295, 24)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Train/val sliding windows\n",
        "x_seq, time_enc_seq, y = create_sliding_windows_with_future_time(\n",
        "    df_trainval, lookback_window, forecast_horizon, feature_cols, time_encoding_cols, y_col)\n",
        "print(\"x_seq_train shape:\", np.shape(x_seq))\n",
        "print(\"time_enc_train shape:\", np.shape(time_enc_seq))\n",
        "print(\"y_train shape:\", np.shape(y))\n",
        "d_model= 256\n",
        "nhead = 8\n",
        "num_layers = 4\n",
        "seq_len = 24\n",
        "dr = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerSeq2SeqForecastModel(15, d_model, nhead, num_layers, num_layers, seq_len, forecast_horizon, dropout = dr).to(device)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 42\n",
        "\n",
        "\n",
        "# Shuffle and split train/val\n",
        "np.random.seed(42)\n",
        "indices = np.arange(len(x_seq))\n",
        "np.random.shuffle(indices)\n",
        "x_seq = x_seq[indices]\n",
        "time_enc_seq = time_enc_seq[indices]\n",
        "y = y[indices]\n",
        "\n",
        "split_idx = int(len(x_seq) * 0.9)\n",
        "x_seq_train, x_seq_val = x_seq[:split_idx], x_seq[split_idx:]\n",
        "time_enc_train, time_enc_val = time_enc_seq[:split_idx], time_enc_seq[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, inputs, y_data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: a list or tuple with two elements:\n",
        "                inputs[0]: historical sequence data (e.g., x_seq)\n",
        "                inputs[1]: future time encoding data (e.g., time_enc)\n",
        "            y_data: target values.\n",
        "        \"\"\"\n",
        "        self.x_data = torch.tensor(inputs[0], dtype=torch.float32)\n",
        "        self.time_data = torch.tensor(inputs[1], dtype=torch.float32)\n",
        "        self.y_data = torch.tensor(y_data, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a tuple:\n",
        "        #   First element: a list containing the two input tensors.\n",
        "        #   Second element: the target tensor.\n",
        "        return [self.x_data[idx], self.time_data[idx]], self.y_data[idx]\n",
        "\n",
        "# Create dataset objects for training and validation.\n",
        "train_dataset = TimeSeriesDataset([x_seq_train, time_enc_train], y_train)\n",
        "val_dataset   = TimeSeriesDataset([x_seq_val, time_enc_val], y_val)\n",
        "\n",
        "# Define DataLoader objects for training and validation.\n",
        "# Adjust batch_size and num_workers as needed.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZkdEXADvqRz",
        "outputId": "213c780c-3acb-4bea-c99b-01ea9f07ea05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train power optimized: 0 batch optimized: 0 freq optimized: 0\n",
            "[1, 1, 1, 1.0, 0.9900498337491681, 0.9801986733067553, 0.9704455335485082, 0.9607894391523232, 0.951229424500714, 0.9417645335842487, 0.9323938199059483, 0.9231163463866358, 0.9139311852712282, 0.9048374180359595, 0.8958341352965282, 0.8869204367171575, 0.8780954309205613, 0.8693582353988059, 0.8607079764250578, 0.8521437889662113, 1.0, 0.9704455335485082, 0.9417645335842487, 0.9139311852712282, 0.8869204367171575, 0.8607079764250578, 0.835270211411272, 0.8105842459701871, 0.7866278610665535, 0.7633794943368531, 0.7408182206817179, 0.7189237334319262, 0.697676326071031, 0.6770568744981647, 0.6570468198150567, 0.6376281516217733, 0.6187833918061408, 0.6004955788122659, 0.5827482523739896, 0.5655254386995371, 0.5488116360940264, 0.5325918010068972, 0.5168513344916993, 0.5015760690660556, 0.4867522559599717, 0.4723665527410147, 0.4584060113052235, 0.44485806622294116, 0.43171052342907973, 0.418951549247639, 0.40656965974059917, 0.39455371037160114, 0.38289288597511206, 0.3715766910220457, 0.3605949401730783, 0.3499377491111553, 0.3395955256449391, 0.3295589610751891, 0.31981902181630395, 0.31036694126548503, 0.30119421191220214, 0.2922925776808594, 0.2836540264997704, 0.27527078308975234, 0.2671353019658504, 0.25924026064589156, 0.2515785530597565, 0.2441432831534371, 0.23692775868212176, 0.22992548518672384, 0.22313016014842982, 0.21653566731600707, 0.21013607120076472, 0.20392561173421347, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# Define the optimizer and loss function.\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.HuberLoss()\n",
        "num_epochs = 100\n",
        "#tracker.start()\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "monitor_b = False\n",
        "if monitor_b:\n",
        "\n",
        "  monitor = ZeusMonitor(gpu_indices=[torch.cuda.current_device()])\n",
        "\n",
        "\n",
        "optimal_batch_size = 64\n",
        "\n",
        "if batch_opt:\n",
        "  job_spec = JobSpec(\n",
        "    job_id=os.environ.get(\"ZEUS_JOB_ID\"),\n",
        "    job_id_prefix=\"transformerEnergyForece\",\n",
        "    default_batch_size=64,\n",
        "    batch_sizes=[16, 32, 64, 128, 256, 512],  # Search space\n",
        "    max_epochs=10,\n",
        "    target_val_metric=0.05,\n",
        "    higher_metric_is_better=False  # lower MSE is better\n",
        "    )\n",
        "  server_url = \"http://bso-server:8000\"  # or whatever the Zeus server address is\n",
        "\n",
        "  bso = BatchSizeOptimizer(\n",
        "    monitor=monitor,\n",
        "    server_url=server_url,\n",
        "    job=job_spec,\n",
        "    rank=0  # rank=0 for non-distributed training\n",
        "  )\n",
        "\n",
        "\n",
        "  optimal_batch_size = bso.get_batch_size()\n",
        "\n",
        "  print(f\"Optimal batch size found: {optimal_batch_size}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plo = None\n",
        "val_losses = []\n",
        "train_losses = []  # Initialize an empty list for training losses\n",
        "real_val_mae= []\n",
        "if(optimize):\n",
        "  plo = GlobalPowerLimitOptimizer(monitor)\n",
        "\n",
        "scheduler = None\n",
        "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=3, min_lr=1e-6)\n",
        "\n",
        "epoch_energy = []\n",
        "avg_steps = []\n",
        "print(f\"Train power optimized: {int(optimize)} batch optimized: {int(batch_opt)} freq optimized: {int(freq_opt)}\")\n",
        "\n",
        "\n",
        "if batch_opt:\n",
        "  bso.on_train_begin()\n",
        "\n",
        "\n",
        "initial_tf_ratio = 1\n",
        "final_tf_ratio = 0.2\n",
        "tf_ratio = []\n",
        "warmup = 3\n",
        "slow = 20\n",
        "decay_rate = 0.03\n",
        "slow_decay = 0.01\n",
        "for epoch in range(num_epochs):\n",
        "  if(epoch <warmup):\n",
        "    teacher_forcing_ratio = 1\n",
        "  elif(epoch < slow):\n",
        "    ratio = initial_tf_ratio * math.exp(-slow_decay * (epoch-warmup))\n",
        "    teacher_forcing_ratio = max(ratio, final_tf_ratio)\n",
        "  else:\n",
        "    ratio = initial_tf_ratio * math.exp(-decay_rate * (epoch-slow))\n",
        "    teacher_forcing_ratio = max(ratio, final_tf_ratio)\n",
        "\n",
        "  tf_ratio.append(teacher_forcing_ratio)\n",
        "print(tf_ratio)\n",
        "last_lr = optimizer.param_groups[0]['lr']\n",
        "weights = torch.linspace(0.1, 1.0, steps=forecast_horizon).to(device) #incase we want to do smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "r06YmPfNk6Ny",
        "outputId": "94c2d817-b25e-46f2-ade9-6364ca450adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train for window 24 d_model 256 nhead 8 depth 4 batch size 64 with scheduler False and initial lr 0.0005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100: 100%|██████████| 370/370 [00:10<00:00, 34.22batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF ratio: 1, Epoch 1/100, Train Loss: 0.0555, Val Loss: 0.1060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100: 100%|██████████| 370/370 [00:11<00:00, 33.51batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF ratio: 1, Epoch 2/100, Train Loss: 0.0488, Val Loss: 0.1473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100: 100%|██████████| 370/370 [00:10<00:00, 33.73batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF ratio: 1, Epoch 3/100, Train Loss: 0.0458, Val Loss: 0.1053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100: 100%|██████████| 370/370 [00:10<00:00, 33.97batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF ratio: 1.0, Epoch 4/100, Train Loss: 0.0445, Val Loss: 0.1106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100: 100%|██████████| 370/370 [00:11<00:00, 31.31batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF ratio: 0.9900498337491681, Epoch 5/100, Train Loss: 0.0486, Val Loss: 0.1058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100:  33%|███▎      | 123/370 [00:04<00:09, 26.55batch/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4fbeacf7c33e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#   if optimize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#       plo.on_step_begin()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(f\"train for window {seq_len} d_model {d_model} nhead {nhead} depth {num_layers} batch size {optimal_batch_size} with scheduler {scheduler != None} and initial lr {last_lr}\")\n",
        "\n",
        "for epoch in range(15):\n",
        "    teacher_forcing_ratio = tf_ratio[epoch]\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # if monitor_b:\n",
        "    #   monitor.begin_window(\"epoch\")  # start epoch-level energy/time window\n",
        "    #   steps = []\n",
        "    # if(optimize):\n",
        "    #   plo.on_epoch_begin()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "    for batch_inputs, batch_y in pbar:\n",
        "    #   if optimize:\n",
        "    #       plo.on_step_begin()\n",
        "\n",
        "    #   if monitor_b:\n",
        "    #       monitor.begin_window(\"step\")  # start step-level window\n",
        "\n",
        "\n",
        "      batch_x, batch_time = batch_inputs\n",
        "      batch_x = batch_x.to(device)\n",
        "      batch_time = batch_time.to(device)\n",
        "      batch_y = batch_y.to(device)\n",
        "\n",
        "      # For Option 1:\n",
        "      # Combine along last dimension:\n",
        "      src = torch.cat([batch_x, batch_time], dim=-1)\n",
        "\n",
        "      tgt_input = batch_y.unsqueeze(-1) + torch.randn_like(batch_y.unsqueeze(-1)) * 0.01\n",
        "      use_tf = (random.random() < teacher_forcing_ratio)\n",
        "      if use_tf:\n",
        "          outputs = model(src, tgt_input)\n",
        "      else:\n",
        "          outputs = model(src, None)\n",
        "\n",
        "      loss = criterion(outputs, batch_y)  # (batch_size, forecast_horizon)\n",
        "      #loss = (loss * weights).mean() # incase want to do weight smoothing\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item() * src.size(0)\n",
        "\n",
        "      # End the step window and record its metrics.\n",
        "      # if monitor_b:\n",
        "      #     step_result = monitor.end_window(\"step\")\n",
        "      #     steps.append(step_result)\n",
        "\n",
        "      # #Let the global power optimizer examine this step\n",
        "      # if optimize:\n",
        "      #    plo.on_step_end()\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "    # if monitor_b:\n",
        "    #   mes = monitor.end_window(\"epoch\")  # complete the epoch window\n",
        "    #   epoch_energy.append(mes)\n",
        "    # if optimize:\n",
        "    #   plo.on_epoch_end()\n",
        "\n",
        "\n",
        "    # if monitor_b:\n",
        "\n",
        "    #   print(f\"Epoch {epoch} consumed {mes.time} s and {mes.total_energy} J.\")\n",
        "\n",
        "    #   avg_time = sum(m.time for m in steps) / len(steps)\n",
        "    #   avg_energy = sum(m.total_energy for m in steps) / len(steps)\n",
        "    #   print(f\"One step took {avg_time:.4f} s and {avg_energy:.4f} J on average.\")\n",
        "\n",
        "    #   avg_steps.append([avg_time, avg_energy])\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "      for batch_inputs, batch_y in val_loader:\n",
        "          batch_x, batch_time = batch_inputs\n",
        "          batch_x = batch_x.to(device)\n",
        "          batch_time = batch_time.to(device)\n",
        "          batch_y = batch_y.to(device)\n",
        "\n",
        "          # For Option 1:\n",
        "          # Combine along last dimension:\n",
        "          src = torch.cat([batch_x, batch_time], dim=-1)\n",
        "\n",
        "\n",
        "          outputs = model(src,None)\n",
        "\n",
        "          loss = criterion(outputs, batch_y)\n",
        "          val_loss += loss.item() * src.size(0)\n",
        "\n",
        "\n",
        "\n",
        "      val_loss /= len(val_dataset)\n",
        "\n",
        "      # if scheduler != None:\n",
        "      #   curr_lr = scheduler.get_last_lr()[0]\n",
        "      #   if curr_lr != last_lr:\n",
        "      #     last_lr = curr_lr\n",
        "      #     print(f\"New learning rate: {curr_lr}\")\n",
        "      #   scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "      print(f\"TF ratio: {teacher_forcing_ratio}, Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "    # if batch_opt:\n",
        "\n",
        "    #   bso.on_evaluate(val_loss)\n",
        "\n",
        "    #   # The BSO server will determine whether to stop training\n",
        "    #   if bso.training_finished:\n",
        "    #     break\n",
        "\n",
        "    # Save the model if validation loss has improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), f\"best_transformer_model_window_{seq_len}_d_{d_model}_nhead_{nhead}_depth_{num_layers}_droput_{dr}_batchsize_{optimal_batch_size}.pth\")\n",
        "        print(\"New best model saved!\")\n",
        "\n",
        "with open(f\"Energy Consumption Data/OPTIMIZER_epoch_energy_transformer_optimized_{int(optimize)}_batch_{int(batch_opt)}_freq_{int(freq_opt)}.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    fieldnames = [\"epoch\", \"time_seconds\", \"energy_joules\", \"avg_step_energy\", \"avg_step_time\", \"Val MSE\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    i = 0\n",
        "    for epoch, mes in enumerate(epoch_energy):\n",
        "        writer.writerow({\n",
        "            \"epoch\": epoch,\n",
        "            \"time_seconds\": mes.time,          # assumes mes.time holds time in seconds\n",
        "            \"energy_joules\": mes.total_energy,  # assumes mes.total_energy holds energy in Joules\n",
        "            \"avg_step_energy\": avg_steps[i][1],\n",
        "            \"avg_step_time\": avg_steps[i][0],\n",
        "            \"Val MSE\": val_losses[i]\n",
        "        })\n",
        "        i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyomgrADfEle"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
        "plt.plot(epochs, val_losses, 'r-o', label='Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('MAE Loss')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, tf_ratio[0:len(train_losses)], 'g-o', label='Teacher Forcing Ratio')\n",
        "ax2.set_ylabel('Teacher Forcing Ratio')\n",
        "ax1.legend()\n",
        "plt.title('Train vs. Validation Loss with tf Ratio on left')\n",
        "ax2.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKk5P1QAd4iE",
        "outputId": "f178d7eb-0052-4e26-c7f0-bff890c3ac89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation RMSE: 20.93526\n",
            "26.755021005584716\n"
          ]
        }
      ],
      "source": [
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "with torch.no_grad():\n",
        "      for batch_inputs, batch_y in val_loader:\n",
        "          batch_x, batch_time = batch_inputs\n",
        "          batch_x = batch_x.to(device)\n",
        "          batch_time = batch_time.to(device)\n",
        "          batch_y = batch_y.to(device)\n",
        "\n",
        "          # For Option 1:\n",
        "          # Combine along last dimension:\n",
        "          src = torch.cat([batch_x, batch_time], dim=-1)\n",
        "\n",
        "\n",
        "          outputs = model(src,None)\n",
        "\n",
        "          loss = criterion(outputs, batch_y)\n",
        "          val_loss += loss.item() * src.size(0)\n",
        "\n",
        "          predictions.append(outputs.cpu().numpy())\n",
        "          actuals.append(batch_y.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)  # (num_val_samples, forecast_horizon)\n",
        "\n",
        "actuals = np.concatenate(actuals, axis=0)          # (num_val_samples, forecast_horizon)\n",
        "#predictions = np.concatenate(predictions, axis=0)  # (num_val_samples, forecast_horizon) # This line was causing issues and is not needed\n",
        "\n",
        "# Reshape predictions_unscaled to match actuals shape before calculating RMSE\n",
        "predictions_unscaled = scaler_target.inverse_transform(predictions.reshape(-1, 1)).reshape(actuals.shape)\n",
        "actuals_unscaled = scaler_target.inverse_transform(actuals.reshape(-1, 1)).reshape(actuals.shape) # Assuming actuals also needs unscaling\n",
        "\n",
        "sample_idx = random.randint(0, len(val_dataset)-1)\n",
        "forecast_horizon = predictions_unscaled.shape[1]\n",
        "\n",
        "forecast_sample = predictions_unscaled[-1]  # shape (24,)\n",
        "\n",
        "# Create an x-axis for the 24 forecast hours: 0..23\n",
        "hours_future = np.arange(0, forecast_horizon)\n",
        "\n",
        "# Calculate RMSE using the reshaped predictions_unscaled\n",
        "rmse = np.sqrt(np.mean((predictions_unscaled - actuals) ** 2))\n",
        "mae = mean_absolute_error(predictions_unscaled, actuals)\n",
        "print(\"Validation RMSE:\", rmse)\n",
        "print(mae)\n",
        "\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# plt.plot(hours_future, forecast_sample, 'o-', label='Forecasted Price')\n",
        "# plt.plot(hours_future, actuals[-1], 'o-', label='Actual Future Price')\n",
        "# plt.title('24-Hour Price Forecast vs Actual')\n",
        "# plt.xlabel('Hour Relative to Forecast Start')\n",
        "# plt.ylabel('Settlement Point Price ($/MWh)')\n",
        "# plt.grid(True)\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# predictions = []\n",
        "# actuals = []\n",
        "# with torch.no_grad():\n",
        "#     for batch_X, batch_y in train_loader:\n",
        "#         batch_X = batch_X.to(device)\n",
        "#         batch_y = batch_y.to(device)\n",
        "\n",
        "#         outputs = model(batch_X)  # shape (batch_size, forecast_horizon)\n",
        "\n",
        "#         # Move back to CPU and store\n",
        "#         predictions.append(outputs.cpu().numpy())\n",
        "#         actuals.append(batch_y.cpu().numpy())\n",
        "\n",
        "# predictions = np.concatenate(predictions, axis=0)  # (num_val_samples, forecast_horizon)\n",
        "# actuals = np.concatenate(actuals, axis=0)          # (num_val_samples, forecast_horizon)\n",
        "\n",
        "# predictions_unscaled = scaler_y.inverse_transform(predictions)\n",
        "# actuals_unscaled = scaler_y.inverse_transform(actuals)\n",
        "\n",
        "# sample_idx = random.randint(0, len(val_dataset)-1)\n",
        "# forecast_horizon = predictions_unscaled.shape[1]\n",
        "\n",
        "# forecast_sample = predictions_unscaled[sample_idx]  # shape (24,)\n",
        "# actual_sample   = actuals_unscaled[sample_idx]      # shape (24,)\n",
        "\n",
        "# # Create an x-axis for the 24 forecast hours: 0..23\n",
        "# hours_future = np.arange(0, forecast_horizon)\n",
        "\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# plt.plot(hours_future, actual_sample, 'o-', label='Actual Future Price')\n",
        "# plt.plot(hours_future, forecast_sample, 'o-', label='Forecasted Price')\n",
        "\n",
        "# plt.title('24-Hour Price Forecast vs Actual for TRAIN SET')\n",
        "# plt.xlabel('Hour Relative to Forecast Start')\n",
        "# plt.ylabel('Settlement Point Price ($/MWh)')\n",
        "# plt.grid(True)\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0cUtUpDywB-"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz\n",
        "!pip install hiddenlayer\n",
        "!pip install torchinfo\n",
        "!pip install graphviz\n",
        "\n",
        "!pip install torchview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoOtctnNzCW2"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "from torchviz import make_dot\n",
        "import io\n",
        "import sys\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchview import draw_graph\n",
        "model = TransformerSeq2SeqForecastModel(input_dim, d_model, nhead, num_layers, num_layers, seq_len, forecast_horizon).to_empty(device=device)\n",
        "\n",
        "model.load_state_dict(torch.load(f\"best_transformer_model_window_{seq_len}_d_{d_model}_nhead_{nhead}_depth_{num_layers}_droput_{dr}_batchsize_{optimal_batch_size}.pth\"))\n",
        "\n",
        "model_graph = draw_graph(model, input_size=[(64, 720, input_dim), (64, 24)], device='cuda')\n",
        "model_graph.visual_graph\n",
        "\n",
        "print(summary(model, input_size=[(64, 720, input_dim), (64, 24)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLReBBNyoXB"
      },
      "outputs": [],
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"tx_torchviz\", format=\"png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4z13nLltfPv"
      },
      "source": [
        "# Transformer Works Poorly, seems like local variables like temp and wind speed are more important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ask1eGH9tdcm"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcxItv4Kd84P"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "input_dim = len(df_transformed.columns)\n",
        "seq_len = 24\n",
        "forecast_horizon = 24\n",
        "target_col_idx = df_transformed.columns.get_loc('Settlement Point Price')\n",
        "\n",
        "X, y = create_sliding_windows_with_future_weather(df_transformed, input_window=seq_len, forecast_horizon=forecast_horizon, target_column='Settlement Point Price')\n",
        "\n",
        "\n",
        "# Step 5: Train/Val split\n",
        "split_idx = int(0.6 * len(X))\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Step 6: Normalize target (y)\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "# Step 7: Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Step 8: Build Datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "# Step 9: Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Debug prints\n",
        "print(\"X_train:\", X_train_tensor.shape, \"y_train:\", y_train_tensor.shape)\n",
        "print(\"X_val:\", X_val_tensor.shape, \"y_val:\", y_val_tensor.shape)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, title=\"Training vs Validation Loss\", save_path=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
        "    plt.plot(val_losses, label=\"Validation Loss\", marker='s')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def train_forecast_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler=None,\n",
        "    clip_grad=0.5,\n",
        "    save_path=\"best_model_no_seq.pth\"\n",
        "):\n",
        "    model.to(device)\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "\n",
        "        for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            batch_X = batch_X.to(device)  # (B, T, F)\n",
        "            batch_y = batch_y.to(device)  # (B, H)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(batch_X)        # (B, H)\n",
        "\n",
        "            loss = criterion(preds, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            if clip_grad:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "        epoch_train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X = batch_X.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                preds = model(batch_X)\n",
        "                loss = criterion(preds, batch_y)\n",
        "                epoch_val_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "        epoch_val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        # ----------------- Logging -----------------\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"New best model saved!\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(epoch_val_loss)\n",
        "\n",
        "    plot_losses(train_losses, val_losses)\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "model = TransformerExogenousForecastModel(\n",
        "    input_dim=11,\n",
        "    d_model=256,\n",
        "    nhead=8,\n",
        "    num_layers=3,\n",
        "    seq_len=24,\n",
        "    forecast_horizon=24\n",
        ")\n",
        "\n",
        "\n",
        "X, y = create_sliding_windows_with_future_weather(combined_df, input_window=24, forecast_horizon=24, target_column='Settlement Point Price')\n",
        "\n",
        "\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "scheduler = None# torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "yhat = model(batch.text) # Give dummy batch to forward().\n",
        "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
        "\n",
        "graph = hl.build_graph(model, batch.text, transforms=transforms)\n",
        "graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "graph.save('tx_hiddenlayer', format='png')\n",
        "\n",
        "make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
        "\n",
        "\n",
        "train_losses, val_losses = train_forecast_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    scheduler=scheduler\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQqL8GFXG_J9"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(f\"best_model_no_seq.pth\"))\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in val_loader:\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        outputs = model(batch_X)  # shape (batch_size, forecast_horizon)\n",
        "\n",
        "        # Move back to CPU and store\n",
        "        predictions.append(outputs.cpu().numpy())\n",
        "        actuals.append(batch_y.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)  # (num_val_samples, forecast_horizon)\n",
        "actuals = np.concatenate(actuals, axis=0)          # (num_val_samples, forecast_horizon)\n",
        "\n",
        "predictions_unscaled = scaler_y.inverse_transform(predictions)\n",
        "actuals_unscaled = scaler_y.inverse_transform(actuals)\n",
        "\n",
        "sample_idx = random.randint(0, len(val_dataset)-1)\n",
        "forecast_horizon = predictions_unscaled.shape[1]\n",
        "\n",
        "forecast_sample = predictions_unscaled[sample_idx]  # shape (24,)\n",
        "actual_sample   = actuals_unscaled[sample_idx]      # shape (24,)\n",
        "\n",
        "# Create an x-axis for the 24 forecast hours: 0..23\n",
        "hours_future = np.arange(0, forecast_horizon)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(hours_future, actual_sample, 'o-', label='Actual Future Price')\n",
        "plt.plot(hours_future, forecast_sample, 'o-', label='Forecasted Price')\n",
        "\n",
        "plt.title('24-Hour Price Forecast vs Actual')\n",
        "plt.xlabel('Hour Relative to Forecast Start')\n",
        "plt.ylabel('Settlement Point Price ($/MWh)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "predictions = []\n",
        "actuals = []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        outputs = model(batch_X)  # shape (batch_size, forecast_horizon)\n",
        "\n",
        "        # Move back to CPU and store\n",
        "        predictions.append(outputs.cpu().numpy())\n",
        "        actuals.append(batch_y.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)  # (num_val_samples, forecast_horizon)\n",
        "actuals = np.concatenate(actuals, axis=0)          # (num_val_samples, forecast_horizon)\n",
        "\n",
        "predictions_unscaled = scaler_y.inverse_transform(predictions)\n",
        "actuals_unscaled = scaler_y.inverse_transform(actuals)\n",
        "\n",
        "sample_idx = random.randint(0, len(val_dataset)-1)\n",
        "forecast_horizon = predictions_unscaled.shape[1]\n",
        "\n",
        "forecast_sample = predictions_unscaled[sample_idx]  # shape (24,)\n",
        "actual_sample   = actuals_unscaled[sample_idx]      # shape (24,)\n",
        "\n",
        "# Create an x-axis for the 24 forecast hours: 0..23\n",
        "hours_future = np.arange(0, forecast_horizon)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(hours_future, actual_sample, 'o-', label='Actual Future Price')\n",
        "plt.plot(hours_future, forecast_sample, 'o-', label='Forecasted Price')\n",
        "\n",
        "plt.title('24-Hour Price Forecast vs Actual for TRAIN SET')\n",
        "plt.xlabel('Hour Relative to Forecast Start')\n",
        "plt.ylabel('Settlement Point Price ($/MWh)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
